{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbe8fpzssoQU"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <h1>NLP et Espace Vectoriel</h1>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<center><i>Réalisé par : </i>Douba JAFUNO </center>\n",
    "\n",
    "<table style=\"width: 100%\">\n",
    "<tr>\n",
    "    <td style=\"width: 15%\">\n",
    "    </td>\n",
    "    <td style=\"width: 70%; text-align:left\">\n",
    "        <a href=\"#1\"><h1>I. Modèle spatiaux vectoriel</h1></a><br>\n",
    "           &nbsp; <a href=\"#wbw\">I.1 Word by Word</a><br>\n",
    "           &nbsp; <a href=\"#wbd\">I.2 Word by document</a><br>\n",
    "           &nbsp; <a href=\"#de\">I.3 Distance euclidienne</a><br>     \n",
    "           &nbsp; <a href=\"#sc\">I.4 Similitude des cosinus</a><br>\n",
    "           &nbsp; <a href=\"#mvm\">I.5 Manipuler la vectorisation de mots (Word embedding) sur les capitales</a><br>\n",
    "           &ensp; <a href=\"#cd1\">I.5.1 Chargement des données</a><br>\n",
    "           &ensp; <a href=\"#tc\">I.5.2 Trouver les capitales</a><br>     \n",
    "           &nbsp; <a href=\"#acp\">I.6 ACP</a><br><br>\n",
    "           \n",
    "\n",
    "\n",
    "<a href=\"#2\"><h1>II. Traduction de mot</h1></a><br><br>\n",
    "    &nbsp; <a href=\"#tc\">II.1 Matrices de Word embedding</a><br>\n",
    "    &nbsp; <a href=\"#ec\">II.2 Autour des données de word embedding anglais et français</a><br>\n",
    "    &ensp; <a href=\"#cd2\">II.2.1 Chargement des données</a><br>\n",
    "    &ensp; <a href=\"#cmwe\">II.2.2 Chargement des matrices de word embedding</a><br>\n",
    "    &ensp; <a href=\"#gmtnf\">II.2.3 Générer la matrice de transformation avec la norme de Frobenius</a><br>\n",
    "    &ensp; <a href=\"#kt\">II.2.4 KNN et tests</a><br><br>\n",
    "\n",
    "  \n",
    "\n",
    "<a href=\"#3\"><h1>III. Matrice et Vecteur de documents (tweets)</h1></a><br><br>\n",
    "    &nbsp; <a href=\"#vd\">III.1 Vecteur de document</a><br>\n",
    "    &nbsp; <a href=\"#idt\">III.2 Importation des données de tweets</a><br>\n",
    "    &nbsp; <a href=\"#cmddt\">III.3 Construction de la matrice de document avec les données de tweets</a><br>\n",
    "    &nbsp; <a href=\"#ttsasc\">III.2.2 Trouver les tweets similaires avec la similarité cosinus</a><br><br>\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"#4\"><h1>IV. Hachage sensible à la localité (HSL)</h1></a><br><br>\n",
    "    &nbsp; <a href=\"#fh\">IV.1 Fonction de Hachage</a><br>\n",
    "    &nbsp; <a href=\"#fhm\">IV.2 Fonction de Hachage multiplan</a><br>\n",
    "    &ensp; <a href=\"#lhdev\">IV.2.1 Les hyperplans dans les espaces vectoriels</a><br>\n",
    "    &ensp; <a href=\"#uhdev\">IV.2.2 Utilisation des hyperplans pour diviser l'espace vectoriel</a><br>\n",
    "    &nbsp; <a href=\"#vhpp\">IV.3 Valeur de hachage avec plusieurs plans</a><br>\n",
    "    &nbsp; <a href=\"#pa\">IV.4 Plans aléatoires</a><br>\n",
    "    &nbsp; <a href=\"#ttpsh\">IV.5 Trouver les tweets les plus similaires avec le HSL</a><br>\n",
    "    &ensp; <a href=\"#cnp\">IV.5.1 Choisir le nombre de plans</a><br>\n",
    "    &ensp; <a href=\"#ovhv\">IV.5.2 Obtenir la valeur de hachage d'un vecteur</a><br>\n",
    "    &ensp; <a href=\"#cth\">IV.5.3 Création d'une table de hachage</a><br>\n",
    "    &ensp; <a href=\"#ctth\">IV.5.4 Création de toutes les tables de hachage</a><br>\n",
    "    &ensp; <a href=\"#akuh\">IV.5.5 Avec KNN en utilisant HSL</a><br><br>\n",
    "\n",
    "<a href=\"#5\"><h1>V. Référence </h1></a><br><br>\n",
    "   </td>\n",
    "    <td style=\"width: 0%\">\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T7Tex0B3soQW"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z8wHfqVpsoQX",
    "outputId": "dc31bc71-01ab-464d-d4a3-4eb85302ad5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = pickle.load(open(\"word_embeddings_subset.p\", \"rb\"))\n",
    "len(word_embeddings)  # there should be 243 words that will be used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l3uxh1BEsoQZ",
    "outputId": "b1a69c7c-22c6-440d-dc13-71320fa386e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"dimension: {}\".format(word_embeddings['Spain'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HRLJ-dHsoQa"
   },
   "source": [
    "# <a name=\"1\">I. Modèle spatiaux vectoriel</a>\n",
    "\n",
    "Les Modèle spatiaux vectoriel: \n",
    "\n",
    "$\\bullet$ Vont nous aider par rapport aux significations des phrases :\n",
    "- where are you heading? ou allez vous   et   were are you from?   d'ou venez vous, ont une signification différente\n",
    "- What are you age ? et How old are you ?   ont une meme signification\n",
    "\n",
    "$\\bullet$ Vont capturer la dépendance entre les mots \n",
    "- You eat cereal from a bowl  (cereal , bowl)\n",
    "- You buy something and someone else sells it (buy,sells)\n",
    "\n",
    "$\\bullet$ Vont trouver les similitudes des mots dans un espace vectoriel \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkoizwzHsoQa"
   },
   "source": [
    "$\\bullet$ Vont trouver les similitudes des mots dans un espace vectoriel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpufrMUZsoQb"
   },
   "source": [
    "## <a name=\"wbw\"> Word by Word </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxFmkpSPsoQb"
   },
   "source": [
    "Pour obtenir un modele spatiaux vectoriel, On va créer une `Matrice de Coocurrence`: comment trouver les similitudes des mots dans un espaces vectoriel:  on désigne un paramètre k qui est la distance entre 2 mots dans une phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUjSlARtsoQb"
   },
   "source": [
    "I like **simple data**. (distance d'un mot, k=1)\n",
    "\n",
    "I prefer **simple** raw **data**. (distance de 2 mots, k=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L44IIS_zsoQc"
   },
   "source": [
    "On fait la matrice co-occurence pour le mot data avec k = 2 pour le mot si pour 2 mots k >2 alors leur coefficient dans la matrice de co-occurence vaudra 0, ici on considère que prefer n'est pas dans notre vocabulaire, en effet cette matrice ne concernera que les mots du vocabulaire "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "gS1Hn3UWsoQc"
   },
   "source": [
    "     simple   raw    like   I  \n",
    "\n",
    "data   2       1      1     0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJWyKHcLsoQd"
   },
   "source": [
    "## <a name=\"wbd\"> Word by document </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtSgvbrLsoQd"
   },
   "source": [
    "On utilisera une matrice de Coocurrence avec cette fois ci le nombre de fois que les mots du vocabulaire apparaissent dans des documents qui appartiennent à des catégories spécifiques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MC5g_X5asoQd"
   },
   "source": [
    "Les coefficients représentent le nombre d'apparition d'un mot dans un document spécifique on parle de taux de similarité "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "bYFQ_vb3soQe"
   },
   "source": [
    "           entertainment     economy    machine learning\n",
    "\n",
    "data          500              6620           9320\n",
    "\n",
    "film          7000             4000           1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ5e0QCVsoQe"
   },
   "source": [
    "<img src=\"images/MSV1.png\" style=\"width:700px;height:400;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LX0r9_TsoQe"
   },
   "source": [
    "ML est plus proche de data quand entertainement est plus proche de film"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GunbZxy4soQf"
   },
   "source": [
    "## <a name=\"de\"> Distance euclidienne</a>\n",
    "\n",
    "\n",
    "\n",
    "Pour calculer la similarité entre deux vecteurs $A=(A_1,A_2,...A_n)$ et $B= (B_1,B_2....B_n)$ on utilise la distance euclidienne entre un point .\n",
    "La distance euclidienne est définie comme suit : \n",
    "\n",
    "$$\\begin{aligned} d(\\mathbf{A}, \\mathbf{B})=d(\\mathbf{B}, \\mathbf{A}) &=\\sqrt{\\left(A_{1}-B_{1}\\right)^{2}+\\left(A_{2}-B_{2}\\right)^{2}+\\cdots+\\left(A_{n}-B_{n}\\right)^{2}} \\\\ &=\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}-B_{i}\\right)^{2}} \\end{aligned}$$\n",
    "\n",
    "<img src=\"images/euc_dist.png\" style=\"width:700px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCBSMFN9soQf"
   },
   "source": [
    "Entrainement correpond au corpus A(500,7000)\n",
    "ML au corpus B(9320,100)\n",
    "alors $d(A,B)=\\sqrt{\\left(A_{1}-B_{1}\\right)^{2}+\\left(A_{2}-B_{2}\\right)^{2}}=\\sqrt{\\left(-8820\\right)^{2}+\\left(6000\\right)^{2}}\\approx 10667$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "BoYh7I2osoQf"
   },
   "source": [
    "                u_t                v_t            w_t\n",
    "               \n",
    "               data             boba           Ice-Cream\n",
    "\n",
    "Ai               6               0                1\n",
    "\n",
    "drink            0               4                6\n",
    "\n",
    "Food             0               6                8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7-dgjpwsoQg"
   },
   "source": [
    "la distance euclidienne entre $u_t$ et $v_t$ pour data et boba est $$d(u_t, v_t)=\\sqrt{\\left(6-0\\right)^{2}+\\left(0-4\\right)^{2}+\\left(0-6\\right)^{2}}=\\sqrt{36+16+36}=\\sqrt{88}\\approx 9,4$$\n",
    "\n",
    "la distance euclidienne entre $w_t$ et $v_t$ pour boba et Ice-Cream est $$d(w_t, v_t)=\\sqrt{\\left(1-0\\right)^{2}+\\left(6-4\\right)^{2}+\\left(8-6\\right)^{2}}=\\sqrt{1+4+4}=\\sqrt{9}=3$$\n",
    "\n",
    "\n",
    "Nous avons 3<9.4 donc le vecteur $v_t$ est plus similaire au vecteur $w_t$ qu'au vecteur $u_t$, la distance euclidienne est plus petite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-eBdb6cPsoQg"
   },
   "outputs": [],
   "source": [
    "def euclidean(A, B):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        d: numerical number representing the Euclidean distance between A and B.\n",
    "    \"\"\"\n",
    "\n",
    "    # d distance euclidienne\n",
    "\n",
    "    d = np.linalg.norm(A-B)\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i5Icl7cMsoQh"
   },
   "outputs": [],
   "source": [
    "king = word_embeddings['king']\n",
    "queen = word_embeddings['queen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Bl78tuJGsoQi",
    "outputId": "8fdc2350-5d5e-45dc-8254-0ea012152228"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4796925"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "euclidean(king, queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czFnmnLAsoQj"
   },
   "source": [
    "## <a name=\"sdc\"> Similitude des cosinus</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAyj1FeQsoQj"
   },
   "source": [
    "La fonction de similarité du cosinus est :\n",
    "\n",
    "$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\\tag{1}$$\n",
    "\n",
    "$A$ et $B$ représentent les mots en vecteur et $A_i$ ou $B_i$ représentent l'indice i de ce vecteur.\n",
    "& Notez que si A et B sont identiques, vous obtiendrez $cos(\\theta) = 1$.\n",
    "* Sinon, s'ils sont totalement opposés, c'est-à-dire, $A= -B$, alors vous obtiendrez $cos(\\theta) = -1$.\n",
    "* Si vous obtenez $cos(\\theta) =0$, cela signifie qu'ils sont orthogonaux (ou perpendiculaires).\n",
    "* Les nombres entre 0 et 1 indiquent un score de similarité.\n",
    "* Les nombres entre -1 et 0 indiquent un score de dissimilarité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pL46fvjsoQk"
   },
   "source": [
    "<img src=\"images/cos_sim.png\" style=\"width:700px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCAWJI9ysoQk"
   },
   "source": [
    "ici $A=(20,40)$ et $B=(30,20)$\n",
    "$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{(20x30) + (40x20)}{\\sqrt{\\left(20\\right)^{2}+\\left(40\\right)^{2}}\\sqrt{\\left(30\\right)^{2}+\\left(20\\right)^{2}}}=0.87$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2wudZvAhsoQl"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Entrée :\n",
    "        A : un tableau numérique qui correspond à un vecteur de mots\n",
    "        B : Un tableau numérique qui correspond à un vecteur de mots\n",
    "    Sortie :\n",
    "        cos : nombre numérique représentant la similitude en cosinus entre A et B.\n",
    "    '''\n",
    "    # you have to set this variable to the true label.\n",
    "    \n",
    "    dot = np.dot(A, B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cN0Ng0GjsoQm",
    "outputId": "4755a8cf-855d-43a6-fba3-3106b8bd11ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510956"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(king, queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT0YLLeKsoQn"
   },
   "source": [
    "## <a name=\"mvm\"> Manipuler la vectorisation de mots (Word embedding) sur les capitales </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DjG_8GzsoQn"
   },
   "source": [
    "Associer une capitale à son pays, à chaque nom de capitale et pays on associe un **embedding**( une vectorisation si vous voulez bien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyZlqWcPsoQn"
   },
   "source": [
    "USA(5,6)         Washington(10,5)\n",
    "\n",
    "Russia(5,5)         Moscow(9,3)\n",
    "\n",
    "Japan(4,5)          Tokyo(8.5,2)\n",
    "\n",
    "Turkey(3,1)        Ankara(8.5,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjzt7jsNsoQn"
   },
   "source": [
    "On veut montrer que Washington est à USA ce que Moscow est à Russia\n",
    "On a Washington - USA = [5,-1]  et Russia + [5,-1] = [10,4]  en regardant notre colonne sauf Washington on remarque que la capitale dont les coordonnées sont plus proche de [10,4] par distance euclidienne est Moscow=[9,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INBsAq7nsoQo"
   },
   "source": [
    "Pour Ankara=[9,1] qu'elle est le pays P le plus similaire ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrS-mc4UsoQo"
   },
   "source": [
    "On a Washington - USA = Ankara - P donc Ankara = P+ Washington-USA =P+[5,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t28pwVbssoQo"
   },
   "source": [
    "On reprend la colonne des pays en ajoutant [5,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOgwl9lgsoQp"
   },
   "source": [
    "USA(10,5)        \n",
    "\n",
    "Russia(10,4)        \n",
    "\n",
    "Japan(9,4)      \n",
    "\n",
    "Turkey(8,0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olfkA4U2soQp"
   },
   "source": [
    "On remarque que la plus petite distance euclidienne d'Ankara=[9,1] est celle avec **Turkey**=[8,0] on a $d=\\sqrt{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYxIiVDaoUiQ"
   },
   "source": [
    "### <a name=\"cd1\"> Chargement des données</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05Xggu3_soQp"
   },
   "source": [
    "On considère des données ville capitale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JHrOySLusoQp",
    "outputId": "0e384f11-2bd6-49ec-e4e3-aa348f5e4681"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>country1</th>\n",
       "      <th>city2</th>\n",
       "      <th>country2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    city1 country1    city2     country2\n",
       "0  Athens   Greece  Bangkok     Thailand\n",
       "1  Athens   Greece  Beijing        China\n",
       "2  Athens   Greece   Berlin      Germany\n",
       "3  Athens   Greece     Bern  Switzerland\n",
       "4  Athens   Greece    Cairo        Egypt"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('capitals.txt', delimiter=' ')\n",
    "data.columns = ['city1', 'country1', 'city2', 'country2']\n",
    "\n",
    "# 5 premieres lignes\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWOjd33WtJyU"
   },
   "source": [
    "### <a name=\"tc\"> Trouver les capitales</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s5LALSLsoQp"
   },
   "source": [
    "Maintenant, nous allons utiliser les fonctions précédentes pour calculer les similitudes entre les vecteurs,\n",
    "et les utiliser pour trouver les capitales des pays. On va implémenter  une fonction `get_country` qui\n",
    "prend trois mots, et le dictionnaire des embeddings. Notre tâche consiste à trouver le\n",
    "les capitales. Par exemple, en donnant les mots suivants : \n",
    "\n",
    "- 1 : Athènes 2 : Grèce 3 : Bagdad,\n",
    "\n",
    "notre tâche consiste à prévoir le pays 4 : l'Irak.\n",
    "\n",
    "**Voici ce que nous ferons** : \n",
    "\n",
    "1. Pour prédire la capitale, nous pouvons nous inspirer de l'exemple *Roi - Homme + Femme = Reine* ci-dessus, et implémenter ce schéma dans une fonction mathématique, en utilisant les words ebedding et une fonction de similarité.\n",
    "\n",
    "2. Itérons le dictionnaire des embeddings et calculons le score de similarité du cosinus entre notre vecteur et l'embedding actuelle du mot.\n",
    "\n",
    "3. Nous devons ajouter une vérification pour nous assurer que le mot à retournez n'est pas l'un des mots que nous avons introduits dans notre fonction. Renvoyons celui qui a obtenu le score le plus élevé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aVLZr8Q2soQq"
   },
   "outputs": [],
   "source": [
    "def get_country(city1, country1, city2, embeddings):\n",
    "    \"\"\"\n",
    "    Entrée :\n",
    "        city1 : une chaîne (la capitale du pays1)\n",
    "        pays1 : une chaîne (le pays de la capitale1)\n",
    "        city2 : une chaîne (la capitale du pays2)\n",
    "        embeddings : un dictionnaire où les clés sont des mots et les valeurs leurs embeddings\n",
    "    Sortie :\n",
    "        pays : un dictionnaire avec le pays le plus probable et son score de similarité\n",
    "    \"\"\"\n",
    "\n",
    "    # stocker la ville 1, le pays 1 et la ville 2 dans un ensemble appelé groupe\n",
    "    group = set((city1, country1, city2))\n",
    "\n",
    "    # Obtenir l'embedding de la ville 1\n",
    "    city1_emb = word_embeddings[city1]\n",
    "\n",
    "    # obtenir l'embedding  du pays 1\n",
    "    country1_emb = word_embeddings[country1]\n",
    "\n",
    "    # obtenir l'embedding  de la ville 2\n",
    "    city2_emb = word_embeddings[city2]\n",
    "\n",
    "    # obtenir l'embedding du pays 2 (c'est une combinaison de l'embedding du pays 1, de la ville 1 et de la ville 2)\n",
    "    # Souvenez-vous : Roi - Homme + Femme = Reine\n",
    "    vec = country1_emb - city1_emb + city2_emb\n",
    "\n",
    "    # Initialisez la similarité à -1 (elle sera remplacée par une similarité plus proche de +1)\n",
    "    similarity = -1\n",
    "\n",
    "    # initialiser le pays à une chaîne vide\n",
    "    country = ''\n",
    "\n",
    "    # boucle sur tous les mots du dictionnaire des embeddings\n",
    "    for word in embeddings.keys():\n",
    "\n",
    "        # vérifiez d'abord que le mot n'est pas déjà dans le \"groupe\".\n",
    "        if word not in group:\n",
    "\n",
    "            # obtenir l'embedding du mot\n",
    "            word_emb = word_embeddings[word]\n",
    "\n",
    "            # calculer la similarité en cosinus entre l'embedding du pays 2 et le mot dans le dictionnaire des embeddings\n",
    "            cur_similarity = cosine_similarity(vec,word_emb)\n",
    "\n",
    "            # si la similarité du cosinus est plus similaire que la meilleure similarité précédente...\n",
    "            if cur_similarity > similarity:\n",
    "\n",
    "                # mettre à jour la similarité avec la nouvelle meilleure similarité\n",
    "                similarity = cur_similarity\n",
    "\n",
    "                # enregistrer le pays comme un tuple, qui contient le mot et la similarité\n",
    "                country = (word, similarity)\n",
    "\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kDFQyTl5soQq",
    "outputId": "8d8c4a0d-8186-4de5-d458-d64e32b72f55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Egypt', 0.7626821)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En testant notre fonction, notons que pour la rendre plus robuste, nous pouvons renvoyer les 5 mots les plus similaires.\n",
    "get_country('Athens', 'Greece', 'Cairo', word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2aakfLusoQq"
   },
   "source": [
    "Athènes est à Grèce ce que Caire est à Egypte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentons `get_accuracy` pour obtenir un score de de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Hucb36HRsoQr"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(word_embeddings, data):\n",
    "    '''\n",
    "    Entrée :\n",
    "        word_embeddings : un dictionnaire dont la clé est un mot et la valeur est son embedding\n",
    "        data : une base de données contenant toutes les paires de pays et de capitales\n",
    "    \n",
    "    Sortie :\n",
    "        accuracy : la précision du modèle\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # initialise num correct à zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # boucler sur les lignes de data\n",
    "    for i, row in data.iterrows():\n",
    "\n",
    "        # récupérer city1\n",
    "        city1 = row['city1']\n",
    "\n",
    "       \n",
    "        country1 = row['country1']\n",
    "\n",
    "        \n",
    "        city2 =  row['city2']\n",
    "\n",
    "        \n",
    "        country2 = row['country2']\n",
    "\n",
    "        # utiliser get_country pour trouver le pays country2\n",
    "        predicted_country2, _ = get_country(city1,country1,city2,word_embeddings)\n",
    "\n",
    "        # si le pays country2 prédit est le même que le pays réel country2...\n",
    "        if predicted_country2 == country2:\n",
    "            # augmenter le nombre de correctes de 1\n",
    "            num_correct += 1\n",
    "\n",
    "    # nombre de ligne de data\n",
    "    m = len(data)\n",
    "\n",
    "    # calculer la précision en divisant le nombre correct par m\n",
    "    accuracy = num_correct/m\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mhdJ5B3GsoQr",
    "outputId": "6b3a874b-c4a9-4d0f-bafd-6b3503842ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision est de 0.92\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(word_embeddings, data)\n",
    "print(f\"La précision est de {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aH-BbWbsoQr"
   },
   "source": [
    "## <a name=\"acp\"> ACP</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ptxc2SJysoQs"
   },
   "source": [
    "Analyse des composantes principale PCA en anglais \n",
    " Il s'agit d'éffectuer une réduction de la dimentionnalité d de nos features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxwEKqDtsoQs"
   },
   "source": [
    "<img src=\"images/acp.png\" style=\"width:700px;height:400;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBsjA9jlsoQs"
   },
   "source": [
    "- On a une représentation de nos données dans un espace vectoriel d'origine d > 2 \n",
    "- Des features non-corrélé pour nos données\n",
    "- On fait une projections des données dans un espace unidimensionnelles(qui conservent le plus d'indormation possible) SVD \n",
    "- Les vecteurs propres indications sur les features non corrélé de la matrice de covariance de nos donnés\n",
    "- Le vecteur propre les variances de nos ensembles de donné quantité d'information retenu par chaque features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tlz1sXnsoQs"
   },
   "source": [
    "Nous allons donc maintenant explorer la distance entre les vecteurs de mots après avoir réduit leur dimension.\n",
    "La technique que nous utiliserons est connue sous le nom de\n",
    "[*analyse en composantes principales* (ACP)](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "Comme nous l'avons vu, nous travaillons dans un espace à 300 dimensions dans ce cas.\n",
    "Bien que d'un point de vue informatique, nous ayons pu faire un bon travail,\n",
    "il est impossible de visualiser les résultats dans des espaces de dimensions aussi élevées.\n",
    "\n",
    "Vous pouvez considérer l'ACP comme une méthode qui projette nos vecteurs dans un espace réduit tout en conservant le maximum d'informations sur les vecteurs originaux dans\n",
    "leurs homologues réduits. Dans ce cas, par *information maximale*, nous entendons que le\n",
    "La distance euclidienne entre les vecteurs originaux et leurs frères et sœurs projetés est de\n",
    "minimal. D'où des vecteurs qui étaient à l'origine proches dans le dictionnaire des encastrements,\n",
    "produira des vecteurs de dimensions inférieures qui sont encore proches les uns des autres.\n",
    "\n",
    "Nous constaterons que lorsque nous tracons la carte des mots, les mots similaires seront regroupés\n",
    "l'un à côté de l'autre. Par exemple, les mots \"triste\", \"heureux\", \"joyeux\" décrivent tous une émotion\n",
    "et sont censés être proches l'un de l'autre lorsqu'ils sont complotés.\n",
    "Les mots : \"gaz\" et \"pétrole\" décrivent tout les deux des ressources naturelles.\n",
    "Des mots comme \"ville\" et \"village\" peuvent être considérés comme des synonymes et décrivent une\n",
    "chose similaire.\n",
    "\n",
    "Avant de tracer les mots, nous devpns d'abord être en mesure de réduire chaque vecteur de mot\n",
    "avec l'ACP en 2 dimensions et le tracer ensuite. Les étapes de calcul de l'ACP sont les suivantes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq8dZO6-soQt"
   },
   "source": [
    "**Instructions** : \n",
    "\n",
    "Nous allons écrire un programme qui prend en compte un ensemble de données où chaque ligne correspond à un vecteur de mots. \n",
    "* Les mots en vecteurs sont de dimension 300. \n",
    "* Utilisez PCA pour changer les 300 dimensions en `n_components` dimensions. \n",
    "* La nouvelle matrice doit être de dimension `m, n_componentns`. \n",
    "\n",
    "* D'abord \"dé-moyenner\" les données\n",
    "* Obtenez les valeurs propres en utilisant \"linalg.eigh\".  Utilisez `eigh` plutôt que `eig` puisque R est symétrique.  Le gain de performance en utilisant \"8\" au lieu de \"8\" est substantiel.\n",
    "* Trier les vecteurs propres et les valeurs propres par ordre décroissant des valeurs propres.\n",
    "* Obtenez un sous-ensemble des vecteurs propres (choisissez combien de composantes principales vous voulez utiliser en utilisant `n_components`).\n",
    "* Retournez la nouvelle transformation des données en multipliant les vecteurs propres par les données d'origine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxjj7WwXsoQt"
   },
   "source": [
    "Pour résumer, le cadre général du calcul de l'ACP est le suivant :\n",
    "\n",
    "- Standardiser les données.\n",
    "- Obtenir les vecteurs propres et les valeurs propres à partir de la matrice de covariance (techniquement la matrice de corrélation après avoir effectué la normalisation).\n",
    "- Trier les valeurs propres par ordre décroissant et choisir les $k$ vecteurs propres qui correspondent aux $k$ valeurs propres les plus importantes , où $k$ est le nombre de dimensions du sous-espace de la nouvelle caractéristique.\n",
    "- Projection sur l'espace du nouvel élément. Au cours de cette étape, nous prendrons les $k$ vecteurs propres les plus importants et les utiliserons pour transformer l'ensemble de données original $X$ afin d'obtenir un sous-espace de caractéristiques à k dimensions $X'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Fa8xU9C1soQt"
   },
   "outputs": [],
   "source": [
    "def compute_pca(X, n_components=2) :\n",
    "    \"\"\"\n",
    "    Entrée :\n",
    "        X : de dimension (m,n) où chaque ligne correspond à un vecteur de mots\n",
    "        n_components : Nombre de composants que vous souhaitez conserver.\n",
    "    Sortie :\n",
    "        X_reduced : données transformées en 2 dims/colonnes + données originales régénérées\n",
    "    \"\"\"\n",
    "\n",
    "    # centrer les données en moyenne\n",
    "    X_demeaned = X - np.mean(X,axis=0)\n",
    "\n",
    "    # calculer la matrice de covariance\n",
    "    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n",
    "\n",
    "    # calculer les vecteurs propres et les valeurs propres de la matrice de covariance\n",
    "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix, UPLO='L')\n",
    "\n",
    "    # trier la valeur propre dans l'ordre croissant (obtenir les indices du tri)\n",
    "    idx_sorted = np.argsort(eigen_vals)\n",
    "    \n",
    "    # inverser l'ordre pour qu'il soit du plus haut au plus bas.\n",
    "    idx_sorted_decreasing = idx_sorted[::-1]\n",
    "\n",
    "    # trier les valeurs propres par idx_sorted_decreasing\n",
    "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
    "\n",
    "    # trier les vecteurs propres en utilisant les indices décroissants idx_sorted_decreasing\n",
    "    eigen_vecs_sorted = eigen_vecs[:,idx_sorted_decreasing]\n",
    "\n",
    "    # sélectionner les n premiers vecteurs propres (n est la dimension souhaitée\n",
    "    # de tableau de données rééchelonnées, ou dims_rescaled_data)\n",
    "    eigen_vecs_subset = eigen_vecs_sorted[:,0:n_components]\n",
    "\n",
    "    # transformer les données en multipliant la transposée des vecteurs propres \n",
    "    # avec la transposition des données dé-moyennées\n",
    "    # ensuite prenez la transposition de ce produit.\n",
    "    X_reduced = np.dot(eigen_vecs_subset.transpose(),X_demeaned.transpose()).transpose()\n",
    "\n",
    "    return X_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3kUnRtLWsoQt",
    "outputId": "0d24bd6d-a296-46c7-844a-2d29d700dbe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your original matrix was (3, 10) and it became:\n",
      "[[ 0.43437323  0.49820384]\n",
      " [ 0.42077249 -0.50351448]\n",
      " [-0.85514571  0.00531064]]\n"
     ]
    }
   ],
   "source": [
    "# Testons notre fonction\n",
    "np.random.seed(1)\n",
    "X = np.random.rand(3, 10)\n",
    "X_reduced = compute_pca(X, n_components=2)\n",
    "print(\"Your original matrix was \" + str(X.shape) + \" and it became:\")\n",
    "print(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zCPc6WhisoQu",
    "outputId": "3fe50157-b5e5-4783-ef33-c1b16ff79ca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous avons 11 mots de  dimensions 300 chacun, donc X.shape est: (11, 300)\n"
     ]
    }
   ],
   "source": [
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town',\n",
    "         'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "# à partir d'une liste de mots et des word embeddings, il retourne une matrice avec tous les embeddings\n",
    "X = get_vectors(word_embeddings, words)\n",
    "\n",
    "print('Nous avons 11 mots de  dimensions 300 chacun, donc X.shape est:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lXFEvEAmsoQu",
    "outputId": "c62dfcc3-3348-463f-a776-4cdfc4274426"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation en 2d\n",
    "result = compute_pca(X, 2)\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0] - 0.05, result[i, 1] + 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfcjVP3OsoQu"
   },
   "source": [
    "# <a name=\"2\">II.Traduction de mot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv0yb9ABsoQv"
   },
   "source": [
    "## <a name=\"mwe\"> Matrices de Word embedding</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZVnHuFCsoQv"
   },
   "source": [
    "On pourrait prendre un mot en anglais et lui associé sa traduction en francais: \n",
    "    Si nous avons cat = [1,1] on parle **d'un word embedding** dans l'espace vectoriel des mots en anglais et que nous avons chat=[2,-2] dans l'espace vectoriel des mots en francais, comment faire le passage de [1,0,1] à [2,3,2]? \n",
    "   - On peut utiliser une matrice de transformation de vecteur que l'on nomme R \n",
    "   - On a $XR\\approx Y$ ou X est une matrice représentant un sous ensemble du Vocabulaire dont les lignes représentent un **embedding**( vecteur associé à un mot) en Anglais et Francais pour Y \n",
    "   - les lignes de X et Y coincident mot en anglais dans X et son mot en francais dans Y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "A0rYuTQBsoQv"
   },
   "outputs": [],
   "source": [
    "import numpy as np                     \n",
    "import matplotlib.pyplot as plt        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "B8ENWq9PsoQv"
   },
   "outputs": [],
   "source": [
    "# fonction qui nous permettra de tracer nos vecteurs \n",
    "def plot_vectors(vectors, colors=['k', 'b', 'r', 'm', 'c'], axes=None, fname='image.svg', ax=None):\n",
    "    scale = 1\n",
    "    scale_units = 'x'\n",
    "    x_dir = []\n",
    "    y_dir = []\n",
    "    \n",
    "    for i, vec in enumerate(vectors):\n",
    "        x_dir.append(vec[0][0])\n",
    "        y_dir.append(vec[0][1])\n",
    "    \n",
    "    if ax == None:\n",
    "        fig, ax2 = plt.subplots()\n",
    "    else:\n",
    "        ax2 = ax\n",
    "      \n",
    "    if axes == None:\n",
    "        x_axis = 2 + np.max(np.abs(x_dir))\n",
    "        y_axis = 2 + np.max(np.abs(y_dir))\n",
    "    else:\n",
    "        x_axis = axes[0]\n",
    "        y_axis = axes[1]\n",
    "        \n",
    "    ax2.axis([-x_axis, x_axis, -y_axis, y_axis])\n",
    "        \n",
    "    for i, vec in enumerate(vectors):\n",
    "        ax2.arrow(0, 0, vec[0][0], vec[0][1], head_width=0.05 * x_axis, head_length=0.05 * y_axis, fc=colors[i], ec=colors[i])\n",
    "    \n",
    "    if ax == None:\n",
    "        plt.show()\n",
    "        fig.savefig(fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctYlm3Q5soQw"
   },
   "source": [
    "dans notre exemple avec cat et chat on a:\n",
    "\n",
    "$$R = \\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}$$\n",
    "\n",
    "Utilisons Numpy pour vérifier que $XR=Y$ on utilisera np.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qoAotMzisoQw"
   },
   "outputs": [],
   "source": [
    "R = np.array([[2, 0],\n",
    "              [0, -2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KMNC8eJysoQw"
   },
   "outputs": [],
   "source": [
    "x = np.array([[1, 1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FbbOJH6MsoQw",
    "outputId": "fa91a566-cc51-4290-c5a9-52c5e9807224"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, -2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(x, R) \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GhxtawwsoQx"
   },
   "source": [
    "On a y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNEln4sRsoQx"
   },
   "source": [
    "Affichage de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Wwj4CdMesoQx",
    "outputId": "848b320e-8419-4c38-a2f0-b6fadcc3cd18"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_vectors([x], axes=[4, 4], fname='transform_x.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CO1_JxDesoQy",
    "outputId": "8e3cf748-dc93-42ca-b422-35d0f926cef7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_vectors([x, y], axes=[4, 4], fname='transformx_and_y.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkOz53aqsoQy"
   },
   "source": [
    "## <a name=\"adweaf\"> Autour des données de word embedding anglais et français</a>\n",
    "le but de cette partie est de rédiger un programme qui traduit l'anglais en français.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEH3VmIgxmVq"
   },
   "source": [
    "### <a name=\"cd2\"> Chargement des données</a>\n",
    "\n",
    "L'ensemble complet des données pour les vectorisations anglaises est d'environ 3,64 gigaoctets, et les données françaises\n",
    "elle est d'environ 629 mégaoctets. On extrait un sous-ensemble des word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSVNtqfgsoQy"
   },
   "source": [
    "<a name=\"dicti\"></a>\n",
    "**Chargeons deux dictionnaires qui font correspondre les mots anglais et français**\n",
    "* Un dictionnaire de train\n",
    "* et un dictionnaire de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "RwfIurt5soQz"
   },
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "VEELORubsoQz"
   },
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    \"\"\"\n",
    "    Cette fonction renvoie le dictionnaire anglais-français dans un fichier où chaque colonne correspond à un mot.\n",
    "    \"\"\"\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        # indexing into the rows.\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-4G01dhIsoQz",
    "outputId": "fbf305bb-8a81-4c89-af5a-784b026875ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longueur du dictionnaire de train anglais-français est de 5000\n",
      "La longueur du dictionnaire de test anglais-français est de 5000\n"
     ]
    }
   ],
   "source": [
    "# chargement des dictionnaires anglais-français\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print('La longueur du dictionnaire de train anglais-français est de', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('La longueur du dictionnaire de test anglais-français est de', len(en_fr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fl8PIvUsoQ0"
   },
   "source": [
    "En regardant le dictionnaire anglais-français\n",
    "\n",
    "* `en_fr_train` est un dictionnaire où la clé est le mot anglais et la valeur\n",
    "est la traduction française de ce mot anglais.\n",
    "```\n",
    "{\"the\" : \"la\",\n",
    " and\" : \"et\",\n",
    " was' : 'était',\n",
    " for : \"pour\",\n",
    "```\n",
    "\n",
    "* `en_fr_test` est similaire à `en_fr_train`, mais c'est un ensemble de tests.  Nous ne le regarderons pas\n",
    "jusqu'à ce qu'on arrive aux tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ42FZ6psoQ0"
   },
   "source": [
    "### <a name=\"gmwe\"> Chargement des matrices de word embedding </a>\n",
    "\n",
    "\n",
    "\n",
    "On va implémenter une fonction `get_matrices`, qui prend les données chargées\n",
    "et renvoie les matrices `X` et `Y`.\n",
    "\n",
    "Entrées :\n",
    "- `en_fr` : Dictionnaire anglais-français\n",
    "- `en_embeddings` : Dictionnaire anglais des embeddings\n",
    "- `fr_embeddings` : Dictionnaire français des embeddings\n",
    "\n",
    "Sorties :\n",
    "- Matrice `X` et matrice `Y`, où chaque ligne de `X` est le  word embedding pour un\n",
    "mot anglais, et la même ligne dans `Y` est le  word embedding pour la version française de ce mot anglais.\n",
    "\n",
    "<div style=\"width:image width px ; font-size:100% ; text-align:center ;\">\n",
    "<img src='images/X_to_Y.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:800px;height:200px ;\" /></div>\n",
    "\n",
    "Utilisons le dictionnaire `en_fr` pour vous assurer que la ième ligne de la matrice `X`\n",
    "correspond à la ième ligne de la matrice `Y`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFxbRp0tsoQ1"
   },
   "source": [
    "la fonction `get_matrices()` :\n",
    "\n",
    "* Itére sur les mots anglais du dictionnaire `en_fr`.\n",
    "* Vérifie si le mot a été vectorisé (embedding) à la fois en anglais et en français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "j4a3COA1soQ1"
   },
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs) :\n",
    "    \"\"\"\n",
    "    Entrée :\n",
    "        en_fr : Dictionnaire anglais-français\n",
    "        french_vecs : Les mots français à leurs word embedding correspondants.\n",
    "        english_vecs : Mots anglais avec leurs word embedding correspondants.\n",
    "    Sortie : \n",
    "        X : une matrice où les colonnes sont les word embedding anglais.\n",
    "        Y : une matrice où les colonnes correspondent aux word embedding français .\n",
    "        R : la matrice de projection qui minimise la norme F ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "    # X_l et Y_l sont des listes de word embedding anglais et français \n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # récupérer les mots anglais (les clés dans le dictionnaire) et les stocker dans un ensemble()\n",
    "    english_set = english_vecs.keys()\n",
    "\n",
    "    # récupérer les mots français (clés dans le dictionnaire) et les stocker dans un ensemble()\n",
    "    french_set = french_vecs.keys()\n",
    "\n",
    "    # stocker les mots français qui font partie du dictionnaire anglais-français (ce sont les valeurs du dictionnaire)\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    # boucler toutes les paires de mots anglais, français dans le dictionnaire anglais-français\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # vérifier que le mot français a une vectorisation (embedding) et que le mot anglais a une embedding\n",
    "         if fr_word in french_set and en_word in english_set:\n",
    "    \n",
    "\n",
    "            # obtenir l'embedding en anglais\n",
    "            en_vec = english_vecs [en_word]\n",
    "\n",
    "            # obtenir l'embedding française\n",
    "            fr_vec = french_vecs [fr_word]\n",
    "\n",
    "            # ajouter l'embedding anglaise à la liste\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # ajouter l'embedding française à la liste\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # empiler les vecteurs de X_l dans une matrice X\n",
    "    X = np.vstack(X_l)\n",
    "\n",
    "    # empiler les vecteurs de Y_l dans une matrice Y\n",
    "    Y = np.vstack(Y_l)\n",
    "   \n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQZZlJ6tsoQ1"
   },
   "source": [
    "Nous allons maintenant utiliser la fonction `get_matrices()` pour obtenir les ensembles `X_train` et `Y_train`.\n",
    "des words embedding anglais et français dans les modèles spatiaux vectoriels correspondants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "7gFRviZbsoQ2"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = get_matrices(\n",
    "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNkA-iTnsoQ2"
   },
   "source": [
    "### <a name=\"gmwetpd\"> Générer la matrice de transformation avec la norme de Frobenius </a>\n",
    "\n",
    "\n",
    "\n",
    "Tout d'abord la norme de Frobenius est la généralisation à $\\mathbf{R}^2$ (espace des Réels au carré) de la fonction de norme déjà connue pour les vecteurs \n",
    "\n",
    "$$\\| \\vec a \\| = \\sqrt {{\\vec a} \\cdot {\\vec a}} $$\n",
    "\n",
    "Pour une matrice A donnée $\\mathbf{R}^2$ , la norme de frobenius est définie comme suit :\n",
    "\n",
    "$$\\|\\mathrm{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oONJwIbOsoQ2"
   },
   "source": [
    "Exemple calculer $\\|\\mathrm{A}\\|_{F}$ avec $A = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WQXEjej6soQ2"
   },
   "outputs": [],
   "source": [
    "A = np.array([[2, 2],\n",
    "              [2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "TBAIpWWmsoQ3",
    "outputId": "06016bef-0553-4c22-9645-5a35657b9721"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4],\n",
       "       [4, 4]], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_squared = np.square(A)\n",
    "A_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "w-ugNd01soQ3",
    "outputId": "660f1e67-e915-4e47-ce6f-09a452ed4a2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_Frobenius = np.sqrt(np.sum(A_squared))\n",
    "A_Frobenius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GauZoky-soQ3"
   },
   "source": [
    "Dans les applications du monde réel, la perte de la norme de Frobenius :\n",
    "\n",
    "$$\\| \\mathbf{XR} - \\mathbf{Y}\\|_{F}$$\n",
    "\n",
    "est souvent remplacée par sa valeur au carré divisée par m:\n",
    "\n",
    "$$ \\frac{1}{m} \\| \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
    "\n",
    "où $m$ est le nombre d'exemples (lignes dans $\\mathbf{X}$).\n",
    "\n",
    "* Le même R est trouvé lorsque l'on utilise cette fonction de perte par rapport à la norme originale de Frobenius.\n",
    "* La raison pour laquelle on prend le carré est qu'il est plus facile de calculer le gradient du carré de Frobenius.\n",
    "* La raison de la division par $m$ est que nous sommes plus intéressés par la perte moyenne par intégration que par la perte pour l'ensemble de la formation.\n",
    "    * La perte pour l'ensemble de la formation augmente avec le nombre de mots (exemples de formation),\n",
    "    Ainsi, le fait de prendre la moyenne nous aide à suivre la perte moyenne, quelle que soit la taille de l'ensemble de formation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uw4QU9V5soQ4"
   },
   "source": [
    "\n",
    "**Comment trouver R ?**\n",
    "\n",
    "On peut utiliser la norme de Foebenius \n",
    "- Initialiser R \n",
    "- faire jusqu'a convergence (Gradient Descent)\n",
    "$$𝐿(𝑋,𝑌,𝑅)=Loss=\\| XR - Y\\|_{F}$$\n",
    "$$g=\\frac{d}{dR}𝐿(𝑋,𝑌,𝑅)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$\n",
    "$$R=R-\\alpha g$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CE8QSdwsoQ4"
   },
   "source": [
    "Calculer la perte\n",
    "* La fonction de perte sera le carré de la norme de Frobenoius de la différence entre\n",
    "et son approximation, divisée par le nombre d'exemples de formation $m$.\n",
    "* Sa formule est la suivante\n",
    "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
    "\n",
    "où $a_{i j}$ est la valeur dans la $i$ème ligne et la $j$ème colonne de la matrice $XR-Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5reJeKTgsoQ4"
   },
   "source": [
    "Fonction `compute_loss()` pour calculer  $L(X, Y, R)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Inpism70soQ5"
   },
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Entrées : \n",
    "        X : une matrice de dimension (m,n) où les colonnes sont les words embedding anglais.\n",
    "        Y : une matrice de dimension (m,n) où les colonnes correspondent aux words embedding français.\n",
    "        R : une matrice de dimension (n,n) - matrice de transformation des words embedding anglais en words embedding français de l'espace vectoriel.\n",
    "    Sortie :\n",
    "        L : une matrice de dimension (m,n) - la valeur de la fonction de perte pour X, Y et R donnés.\n",
    "    '''\n",
    "   \n",
    "    # m est le nombre de lignes de X\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # diff est XR - Y\n",
    "    diff = np.dot(X,R)-Y\n",
    "\n",
    "    # diff_squared est le carré de la différence entre les éléments\n",
    "    diff_squared = diff**2\n",
    "\n",
    "    # sum_diff_squared est la somme des éléments au carré\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # perte: sum_diff_squared divisée par le nombre d'exemples (m)\n",
    "    loss = sum_diff_squared/m\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYR9466ZsoQ5"
   },
   "source": [
    "* On va maintenant calculer le gradient g de la perte par rapport à la matrice de transformation `R`.\n",
    "* Le gradient est une matrice qui modifie la valeur d'un petit changement dans `R`.\n",
    "affectent la modification de la fonction de perte.\n",
    "* Le gradient nous donne la direction dans laquelle nous devrions diminuer `R`.\n",
    "pour minimiser la perte.\n",
    "* $m$ est le nombre d'exemples de train (nombre de lignes en $X$).\n",
    "* La formule pour le gradient de la fonction de perte $𝐿(𝑋,𝑌,𝑅)$ est :\n",
    "\n",
    "$$g=\\frac{d}{dR}𝐿(𝑋,𝑌,𝑅)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$\n",
    "\n",
    "On implémente la fonction `compute_gradient` ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AHBgwkJ9soQ5"
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Entrées : \n",
    "        X : une matrice de dimension (m,n) où les colonnes sont les words embedding anglais.\n",
    "        Y : une matrice de dimension (m,n) où les colonnes correspondent aux words embedding français.\n",
    "        R : une matrice de dimension (n,n) - matrice de transformation des words embedding anglais en words embedding français de l'espace vectoriel.\n",
    "    Sorties :\n",
    "        g : une matrice de dimension (n,n) - gradient de la fonction de perte L pour X, Y et R donnés.\n",
    "    '''\n",
    "    \n",
    "    # m nombre de ligne de X\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # le gradient g est X^T(XR - Y) * 2/m\n",
    "    gradient = np.dot(X.transpose(),np.dot(X,R)-Y)*(2/m)\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27ov7ZavsoQ5"
   },
   "source": [
    "On va implémenter la fonction `align_embeddings` pour la mise à jour par Gradient Descent $$R=R-\\alpha g$$  ou $\\alpha$ est le taux d'apprentissage\n",
    "\n",
    "* Le taux d'apprentissage ou `taille de l'échelon` $\\alpha$ est un coefficient qui décide combien nous voulons changer $R$ à chaque échelon.\n",
    "* Si nous changeons trop $R$, nous pourrions sauter l'optimum en prenant un pas trop grand.\n",
    "* Si nous ne changeons que légèrement $R$, nous aurons besoin de plusieurs étapes pour atteindre l'optimum.\n",
    "* Le taux d'apprentissage $\\alpha$ est utilisé pour contrôler ces changements.\n",
    "* Les valeurs de $\\alpha$ sont choisies en fonction du problème, et nous utiliserons `learning_rate`$=0.0003$ comme valeur par défaut pour notre algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Wbnr3NItsoQ6"
   },
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    '''\n",
    "    Entrées :\n",
    "        X : une matrice de dimension (m,n) où les colonnes sont les words embedding anglais.\n",
    "        Y : une matrice de dimension (m,n) où les colonnes correspondent aux words embedding français.\n",
    "        train_steps : int positif - décrit le nombre de pas que fera l'algorithme de descente en gradient.\n",
    "        learning_rate : positive float - décrit la taille des pas de l'algorithme de descente de gradient.\n",
    "    Sorties :\n",
    "        R : une matrice de dimension (n,n) - la matrice de projection qui minimise la norme F ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # le nombre de colonnes de X est le nombre de dimensions pour un vecteur de mots (par exemple 300)\n",
    "    # R est une matrice carrée dont la longueur est égale au nombre de dimensions de le word enbedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "       \n",
    "        # utilise la fonction définie pour calculer le gradient\n",
    "        gradient = compute_gradient(X,Y,R)\n",
    "\n",
    "        # mettre à jour R en soustrayant le taux d'apprentissage fois le gradient\n",
    "        R -= learning_rate * gradient\n",
    "\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnm39sh-soQ6"
   },
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "124cMSHXsoQ6",
    "outputId": "84b90f41-ad79-46b0-be92-a4940cf2d4de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 3.7242\n",
      "loss at iteration 25 is: 3.6283\n",
      "loss at iteration 50 is: 3.5350\n",
      "loss at iteration 75 is: 3.4442\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AizKB0z0soQ6"
   },
   "source": [
    "calcul de R sur nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-uUCPdCmsoQ7",
    "outputId": "e4e2d7d2-19ee-4a32-f9d4-9970b9b738a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 963.0146\n",
      "loss at iteration 25 is: 97.8292\n",
      "loss at iteration 50 is: 26.8329\n",
      "loss at iteration 75 is: 9.7893\n",
      "loss at iteration 100 is: 4.3776\n",
      "loss at iteration 125 is: 2.3281\n",
      "loss at iteration 150 is: 1.4480\n",
      "loss at iteration 175 is: 1.0338\n",
      "loss at iteration 200 is: 0.8251\n",
      "loss at iteration 225 is: 0.7145\n",
      "loss at iteration 250 is: 0.6534\n",
      "loss at iteration 275 is: 0.6185\n",
      "loss at iteration 300 is: 0.5981\n",
      "loss at iteration 325 is: 0.5858\n",
      "loss at iteration 350 is: 0.5782\n",
      "loss at iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "SM2nNzzBsoQ7",
    "outputId": "f350fc8d-c963-48aa-ab00-680d2dba7de7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01362498, -0.01376288, -0.00382809, ...,  0.00297988,\n",
       "        -0.00585451, -0.00035307],\n",
       "       [ 0.02115732, -0.00386311,  0.00583104, ...,  0.02312933,\n",
       "        -0.00235515,  0.00306189],\n",
       "       [ 0.00820657,  0.00297308,  0.01993382, ..., -0.00215262,\n",
       "        -0.00264195,  0.00812032],\n",
       "       ...,\n",
       "       [-0.00135703,  0.00797791, -0.01720783, ..., -0.00984865,\n",
       "         0.00672823,  0.00275493],\n",
       "       [ 0.00619364, -0.00598647,  0.0027768 , ..., -0.00575446,\n",
       "         0.00292054,  0.00327975],\n",
       "       [ 0.0053762 , -0.01577644, -0.00707282, ..., -0.01446361,\n",
       "         0.00845145, -0.00706754]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpnwEixesoQ7"
   },
   "source": [
    "On pourrait aussi utiliser les [K-plus proches](https://fr.wikipedia.org/wiki/M%C3%A9thode_des_k_plus_proches_voisins) pour trouver un mot francais correspondant mais il n'y a pas de garantie comme celle de trouver ces amis les plus proce dans un meme pays mais si ce n'est pas dans un continent on a plus de chance. Alors pourquoi ne pas placer chacun de nos amis dans des sceaux? parlons des Hash Tables (Regrouper dans des sceaux par similitude) et fonction de Hashage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcMpejrRsoQ7"
   },
   "source": [
    "### <a name=\"kt\"> KNN et tests </a>\n",
    "\n",
    "Comme nous approchons la fonction de traduction des embeddings de l'anglais au français par une matrice de transformation linéaire $R$, la plupart du temps nous n'obtiendrons pas l'embedding exacte d'un mot français lorsque nous transformons l'embedding $\\mathbf{e}$ d'un mot anglais particulier dans l'espace d'embeddings français. \n",
    "* C'est là que $k$-NN devient vraiment utile ! En utilisant $1$-NN avec $eR$ comme entrée, nous pouvons rechercher une embeddings $\\mathbf{f}$ (en ligne) dans la matrice $\\mathbf{Y}$ qui est la plus proche du vecteur transformé $eR$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZAqCxz6soQ8"
   },
   "source": [
    "Entrées :\n",
    "* Vecteur `v`,\n",
    "* Un ensemble de voisins les plus proches possibles `candidats`.\n",
    "* `k` les plus proches voisins à trouver.\n",
    "* La métrique de la distance doit être basée sur la similarité du cosinus.\n",
    "* La fonction de similarité cosinus est déjà implémentée et importée pour vous. Ses arguments sont deux vecteurs et elle renvoie le cosinus de l'angle entre eux.\n",
    "* On itère les lignes dans les `candidats`, et on sauvegarde le résultat des similarités entre la ligne courante et le vecteur `v` dans une liste python. On doit faire attention à ce que les similarités soient dans le même ordre que les vecteurs de lignes des `candidats`.\n",
    "* On peut maintenant utiliser [numpy argsort]( https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy.argsort) pour trier les indices des lignes de `candidats`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcOLfwG4-CY_"
   },
   "source": [
    "Implémentons la fonction `nearest_neighbor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_M6wgxaFsoQ8"
   },
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1) :\n",
    "    \"\"\"\n",
    "    Entrée :\n",
    "      - v, le vecteur pour lequel vous allez trouver le plus proche voisin\n",
    "      - les candidats : un ensemble de vecteurs où l'on trouvera les voisins\n",
    "      - k : top k des voisins les plus proches à trouver\n",
    "    Sortie :\n",
    "      - k_idx : les indices des k vecteurs les plus proches sous forme triée\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_l = []\n",
    "\n",
    "    # pour chaque vecteur candidat...\n",
    "    for row in candidates:\n",
    "        # obtenir la similarité du cosinus\n",
    "        cos_similarity = cosine_similarity(v,row)\n",
    "\n",
    "        # ajouter la similarité à la liste\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    # trier la liste de similitudes et obtenir les indices de la liste triée\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "\n",
    "    # obtenir les indices des k vecteurs candidats les plus similaires\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    \n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-03HmHT7soQ9"
   },
   "source": [
    "voici les 3 plus proches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "EGtuCQFesoQ8",
    "outputId": "76816b45-a0f6-4c8b-fdae-2b18a8e1dbdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 9 9]\n",
      " [1 0 5]\n",
      " [2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixovSnUksoQ9"
   },
   "source": [
    "Implémentons maintenant la fonction `test_vocabulary` qui prend en compte la matrice des words embeddings anglais $X$, la matrice des words embeddings français $Y$ et le $R$ et renvoie la précision des traductions de $X$ à $Y$ par $R$.\n",
    "\n",
    "* Itérer les words embeddings anglais transformés et vérifié si le\n",
    "words embeddings français le plus proche appartient au mot français qui est la traduction réelle.\n",
    "* Obtenir un index de l'incrustation française la plus proche en utilisant\n",
    "`nearest_neighbor` (avec l'argument `k=1`), et le comparer à l'index\n",
    "du word embedding anglais que vous venez de transformer.\n",
    "* Notez le nombre de fois que vous obtenez la bonne traduction.\n",
    "* Calculer la précision comme suit : $$\\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "54-dDdOesoQ9"
   },
   "outputs": [],
   "source": [
    "# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_vocabulary(X, Y, R):\n",
    "    '''\n",
    "    Entrée :\n",
    "        X : une matrice où les colonnes sont les words embedding anglais.\n",
    "        Y : une matrice où les colonnes correspondent aux words embedding français.\n",
    "        R : la matrice de transformation qui traduit les words embedding de l'espace vectoriel de mots anglais-français.\n",
    "    Sortie :\n",
    "        accuracy (précision) : pour les capitales de l'anglais vers le français\n",
    "    '''\n",
    "    # La prédiction est X fois R\n",
    "    pred = np.dot(X,R)\n",
    "\n",
    "    # initialiser le num_correct à zéro\n",
    "    num_correct = 0\n",
    "\n",
    "    # boucle à travers chaque ligne dans le pred (chaque embedding transformé)\n",
    "    for i in range(len(pred)):\n",
    "        # obtenir l'indice du plus proche voisin de pred à la ligne \"i\" ; passer également les candidats à Y\n",
    "        pred_idx = nearest_neighbor(pred[i],Y)\n",
    "\n",
    "       # si l'indice du plus proche voisin est égal à la ligne de i... \\\n",
    "        if pred_idx == i:\n",
    "            # incrémenter le nombre correct de 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # La précision est le nombre correct divisé par le nombre de lignes dans \"pred\" (également le nombre de lignes dans X)\n",
    "    accuracy = num_correct / len(pred)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX04PZZxsoQ9"
   },
   "source": [
    "Voyons comment fonctionne notre mécanisme de traduction sur les données non visibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "rg7Y5b4vsoQ-"
   },
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "3h3TO8_wsoQ-",
    "outputId": "2ec667d7-184c-4085-cbd8-d028d0cf338d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La précision de l'ensemble de tests est 0.557\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)  # cela pourrait prendre une minute ou deux\n",
    "print(f\"La précision de l'ensemble de tests est {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYOHPHTUsoQ-"
   },
   "source": [
    "# <a name=\"3\">III.Matrice et Vecteur de documents (tweets)</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teqjpLAFbsGE"
   },
   "source": [
    "## <a name=\"vd\">Vecteur de document</a>\n",
    "\n",
    "On peut représenter un document( ici il s'agira d'un ensemble de `tweet`) comme un vecteur en additionnant les vecteurs de mots pour les mots à l'intérieur du document. Dans cet exemple, notre **\"word embedding\"** ( le mot et son vecteur si on se réfère à la partie précédente) ne contient que trois mots, chacun étant représenté par un tableau 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvuXPpxPsoQ-"
   },
   "source": [
    "<img src=\"images/doc.png\" style=\"width:700px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVYFamMDsoQ_"
   },
   "source": [
    "Voici le code nous permettant de représenter l'embedding de I love learng!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "lbAGXAAfsoQ_",
    "outputId": "c3c7df0e-1a5a-4faa-c765-9976185e0812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3]\n"
     ]
    }
   ],
   "source": [
    "word_embedding = {\"I\": np.array([1,0,1]),\n",
    "                   \"love\": np.array([-1,0,1]),\n",
    "                   \"learning\": np.array([1,0,1])\n",
    "                  }\n",
    "words_in_document = ['I', 'love', 'learning', 'not_a_word']\n",
    "document_embedding = np.array([0,0,0])\n",
    "for word in words_in_document:\n",
    "    document_embedding += word_embedding.get(word,0)\n",
    "    \n",
    "print(document_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qT2uVEdZwqI"
   },
   "source": [
    "## <a name=\"idt\"> Importation des données de tweets</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_CT_ybdRoDk"
   },
   "source": [
    "On utilise la base de donnée **twitter_samples** qui contient des tweets en anglais ayant des sentiments positifs et négatifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "wWo1RzP-soQ_"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "m_y9x37PsoRA"
   },
   "outputs": [],
   "source": [
    "# récupérer les tweets positifs and negatifs \n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccoFI-6_eIc5"
   },
   "source": [
    "## <a name=\"cmddt\"> Construction de la matrice de document avec les données de tweets</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX1FfUa4Dowp"
   },
   "source": [
    "Nous allons implémenter plusieurs fonction pour pouvoir représenter l'ensemble des vecteurs de chaque teweet en une matrice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dANq7Etc-eS3"
   },
   "source": [
    "Fonction ` process_tweet()`qui renvoie un tweet tokénisé (vecteur qui contients les mots \"importants\" du tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "azgzgdFSsoQ_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Fonction  process_tweet.\n",
    "    Entrée :\n",
    "        tweet : une chaîne de caractères contenant un tweet\n",
    "    Sortie :\n",
    "        tweets_clean : une liste de mots contenant le tweet traité\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # supprimer les tickers etc comme $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # supprimer l'ancien texte retweet \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # supprimer les hyperliens\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # supprimer les hashtags\n",
    "    # en retirant seulement le signe dièse # du mot\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenizer les tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # supprimer les stopwords\n",
    "                word not in string.punctuation):  # supprimer les ponctuations\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # racine du mot\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYAE6wlkBx66"
   },
   "source": [
    "Fonction `get_document_embedding` qui calcule la somme des Word embediing de tweet pour obtenir l'embedding des documents. On utilisera les words enbedding de la [partie précédente](#dicti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "194tk1OSsoRA"
   },
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet, en_embeddings): \n",
    "    '''\n",
    "    Entrée :\n",
    "        - tweet : une chaîne de caractere\n",
    "        - en_embeddings : un dictionnaire de word embedding (mot,vecteur)\n",
    "    Sortie :\n",
    "        - doc_embedding : somme de tout les word embedding pour les mots du tweet tokénisé\n",
    "    '''\n",
    "    doc_embedding = np.zeros(300)\n",
    "\n",
    "    # transformer le document (tweet) en une liste de mots (traiter le tweet)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    for word in processed_doc:\n",
    "        # ajouter le word embeding au total courant pour l'embedding (la vectorisation) du document\n",
    "        doc_embedding += en_embeddings.get(word,0)\n",
    " \n",
    "    return doc_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "IhjexyrdsoRA",
    "outputId": "49654e74-7450-4514-92a1-db1ec6692ddf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test de notre fonction avec un tweet on affiche les 5 derniers\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings_subset)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6l-Pt8aFQWZ8"
   },
   "source": [
    "Tweet tokénisé:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "T1F-4dfwH9kG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'great', 'day', ':)', 'good', 'morn']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet(custom_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQNgFIhuQbvF"
   },
   "source": [
    "Fonction `get_document_vecs` pour avoir l'embedding de chaque tweet de nos données **twitter_samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Y5l-l5hzsoRB"
   },
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "    '''\n",
    "    Entrée :\n",
    "        - all_docs : liste des chaînes de caractères de tous les tweets de notre base de données.\n",
    "        - en_embeddings : dictionnaire dont les mots sont les clés et les valeurs sont leurs embeddings.\n",
    "    Sortie :\n",
    "        - document_vec_matrix : matrice des word embedding de document (tweet).\n",
    "        - ind2Doc_dict : dictionnaire avec les indices des tweets en vecteurs comme clés et leurs valeurs.\n",
    "    '''\n",
    "\n",
    "    # la clé du dictionnaire est un index (entier) qui identifie un tweet spécifique\n",
    "    # la valeur est l'encastrement du document pour ce document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # c'est la liste qui permettra de stocker les vecteurs de documents\n",
    "    document_vec_l = []\n",
    "\n",
    "    for i, doc in enumerate(all_docs):\n",
    "\n",
    "        # obtenir l'embedding du document dans le tweet\n",
    "        doc_embedding = get_document_embedding(doc,en_embeddings)\n",
    "\n",
    "        # enregistrer l'embedding du document dans le dictionnaire ind2Tweet à l'index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "\n",
    "        # ajouter l'embedding du document dans la liste des vecteurs de document\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "\n",
    "    # convertir la liste des vecteurs de documents en un tableau 2D (chaque ligne est un vecteur de document)\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "-zKCCJeMsoRB"
   },
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "GDNG3lVVsoRB",
    "outputId": "913c6cfa-dd22-4e06-b69a-4ecc50e9a74e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur du dictionnaire 10000\n",
      "taille de document_vecs  (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"longueur du dictionnaire {len(ind2Tweet)}\")\n",
    "print(f\"taille de document_vecs  {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "zNro6JdhE3bG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04821777, -0.01904297,  0.15820312, -0.18847656, -0.11352539])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vecs[0,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SKK2w9wa7sB"
   },
   "source": [
    "## <a name=\"ttsasc\"> Trouver les tweets similaires avec la similarité cosinus</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdYb0la6soRB"
   },
   "source": [
    "\n",
    "Nous avons maintenant un vecteur de dimension (m,d) où \"m\" est le nombre de tweets (10 000) et \"d\" est la dimension des embeddings (300).  Nous allons maintenant entrer un tweet, et utiliser la similarité cosinus pour voir quel tweet dans notre corpus lui est similaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "1b3I6XJysoRC"
   },
   "outputs": [],
   "source": [
    "my_tweet = 'i am sad'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Rzuq69AJsoRC",
    "outputId": "45cc225f-5708-478f-efe6-d25e9bf63072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@zoeeylim sad sad sad kid :( it's ok I help you watch the match HAHAHAHAHA\n"
     ]
    }
   ],
   "source": [
    "# cela nous donne un tweet similaire à votre contribution.\n",
    "# cette implémentation est vectorisée...\n",
    "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj1yJdl4soRE"
   },
   "source": [
    "# <a name=\"4\">IV. [Hachage sensible à la localité (HSL)](https://fr.qaz.wiki/wiki/Locality-sensitive_hashing)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkLUCKKWsoRC"
   },
   "source": [
    "## <a name=\"fh\">Fonction de Hashage </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCbyxhFqsoRC"
   },
   "source": [
    "Ici nos \"amis\" sont les vecteur de mot (word embedding)\n",
    "\n",
    "La fonction de hashage: c'est la fonction qui va nous permettre de regrouper nos vecteurs dans des sceaux.\n",
    "\n",
    "Soit 10 sceaux numérotées de 0 à 9 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhaCa49QsoRD"
   },
   "source": [
    "\n",
    "Soit 10 sceaux numérotées de 0 à 9 \n",
    "\n",
    "on associe 100 et 10 au sceaux 0 \n",
    "On associe 14 au sceau 4\n",
    "On associe 17 au sceau 7\n",
    "On associe 97 au sceau 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I7VVkh8soRD"
   },
   "source": [
    "\n",
    "ici l'opérattion de hachage est définit par  Fonction_hachage(Value)= Hash Value avec Hash_Value = Value %  n_buckets(nombre de sceau)  avec % qui signifie modulo, le reste de la division euclidienne de vector par le nombre de sceau qui est de 10, exempe  en effet 14%10 =4 car 10*1+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "kACKyfrksoRD"
   },
   "outputs": [],
   "source": [
    "import pprint                     \n",
    "pp = pprint.PrettyPrinter(indent=4) # jolie couleur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLX6G2Zewbvn"
   },
   "source": [
    "Implémentons cette exemple avec la fonction `basic_hash_table().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "TV2Jcem8soRD"
   },
   "outputs": [],
   "source": [
    "def basic_hash_table(value_l, n_buckets):\n",
    "    \n",
    "    def hash_function(value, n_buckets):\n",
    "        return int(value) % n_buckets\n",
    "    \n",
    "    hash_table = {i:[] for i in range(n_buckets)} # Initialiser tous les seaux dans la table de hachage comme des listes vides\n",
    "\n",
    "    for value in value_l:\n",
    "        hash_value = hash_function(value,n_buckets) # Obtenir la clé de hachage pour la valeur donnée\n",
    "        hash_table[hash_value].append(value) # Ajouter l'élément au seau correspondant\n",
    "    \n",
    "    return hash_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "w01peQlKsoRE",
    "outputId": "2059eea0-9fda-4b57-f037-e6f89b1929db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: [100, 10],\n",
      "    1: [],\n",
      "    2: [],\n",
      "    3: [],\n",
      "    4: [14],\n",
      "    5: [],\n",
      "    6: [],\n",
      "    7: [17, 97],\n",
      "    8: [],\n",
      "    9: []}\n"
     ]
    }
   ],
   "source": [
    "value_l = [100, 10, 14, 17, 97] # Set of values to hash\n",
    "hash_table_example = basic_hash_table(value_l, n_buckets=10)\n",
    "pp.pprint(hash_table_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuZ6wyoksoRF"
   },
   "source": [
    "## <a name=\"fhm\">Fonction de Hachage multiplan </a>\n",
    "\n",
    "Les fonctions de hachage multiplans sont d'autres types de fonctions de hachage. Les fonctions de hachage multiplans sont basées sur l'idée de numéroter chaque région formée par l'intersection de n plans ont les considères alors comme nos sceaux à remplir. Dans le code suivant, nous montrons les formes les plus fondamentales du principe des multiplans. Tout d'abord, avec un seul plan définit par le vecteur normal P= [1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "lFKpefqTsoRF",
    "outputId": "c43480ae-6881-4f55-ece8-69b8129cee63"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "P = np.array([[1, 1]]) # Définir un plan unique. \n",
    "fig, ax1 = plt.subplots(figsize=(8, 8)) #la taille de la figure \n",
    "\n",
    "plot_vectors([P], axes=[2, 2], ax=ax1) # Tracez le plan P comme un vecteur\n",
    "\n",
    "# Tracer des points au hasard. \n",
    "for i in range(0, 10):\n",
    "        v1 = np.array(np.random.uniform(-2, 2, 2)) # Obtenez une paire de nombres aléatoires entre -4 et 4 \n",
    "        side_of_plane = np.sign(np.dot(P, v1.T)) \n",
    "        \n",
    "       # Colorer les points en fonction du signe du résultat de np.dot(P, point.T)\n",
    "        if side_of_plane == 1:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'bo') # point bleu\n",
    "        else:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'ro') # point rouge\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suXhwRVFsoRF"
   },
   "source": [
    "La première chose à noter est que le vecteur qui définit le plan ne marque pas la limite entre les deux côtés du plan. Il marque la direction dans laquelle vous trouvez le côté \"positif\" du plan. Ce n'est pas du tout intuitif !\n",
    "\n",
    "Si nous voulons tracer le plan de séparation, nous devons tracer une ligne qui est perpendiculaire à notre vecteur `P`. Nous pouvons obtenir une telle ligne en utilisant une matrice de rotation de $90^o$. Avant celà voyons un peu les hyperplans et les matrices de rotation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26gPV2kmsoRG"
   },
   "source": [
    "### <a name=\"lhdev\">Les hyperplans dans les espaces vectoriels</a>\n",
    "\n",
    "* Dans un espace vectoriel à $3$, l'hyperplan est un plan régulier. Dans un espace vectoriel à $2$  dimension, l'hyperplan est une ligne.\n",
    "* En général, l'hyperplan est un sous-espace qui a une dimension inférieure de $1$ à celle de l'espace vectoriel d'origine.\n",
    "* Un hyperplan est uniquement défini par son vecteur normal.\n",
    "* Le vecteur normal $n$ du plan $\\pi$ est le vecteur auquel tous les vecteurs du plan $\\pi$ sont orthogonaux (perpendiculaire dans le cas dimensionnel $3$).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oBWBeO79j26"
   },
   "source": [
    "### <a name=\"uhdev\">Utilisation des hyperplans pour diviser l'espace vectoriel</a>\n",
    "\n",
    "Nous pouvons utiliser un hyperplan pour diviser l'espace vectoriel en  $2$ parties:\n",
    "* Tous les vecteurs dont le produit des points avec le vecteur normal d'un plan est positif se trouvent sur un côté du plan.\n",
    "* Tous les vecteurs dont le produit des points avec le vecteur normal du plan est négatif se trouvent de l'autre côté du plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HnrRm7msoRG"
   },
   "source": [
    "Les `matrices de rotation` dans $R^2$ font pivoter un vecteur donné $\\vec x$ d'un angle dans le sens anti-horaire $\\theta$ dans un système de coordonnées fixe. Les matrices de rotation sont de la forme :\n",
    "\n",
    "$$Ro = \\begin{bmatrix} cos \\theta & -sin \\theta \\\\\\\\\\ sin \\theta & cos \\theta \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Maintenant, traçons dans le même système notre vecteur $\\vec x = [1, 1]$ (il s'agit de P) et son produit matriciel avec la matrice\n",
    "$Ro$ pour un angle de 45°\n",
    "\n",
    "Vecteur rotatif\n",
    "\n",
    "$$y = x \\cdot Ro $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "wenYsKhosoRG",
    "outputId": "0e43275a-643f-4749-d674-8ff422eac44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de Rotation\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "\n",
      "Vecteur rotatif\n",
      "[[1.41421356 0.        ]]\n",
      "\n",
      " nome x2 1.4142135623730951\n",
      "\n",
      " norme y2 1.4142135623730951\n",
      "\n",
      " norme de la matrice de Roation 1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "angle = 45 * (np.pi / 180) #convertir 45° en radians\n",
    "\n",
    "Ro = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "              [np.sin(angle), np.cos(angle)]])\n",
    "\n",
    "x2 = np.array([1, 1]).reshape(1, -1) # le transformer en vecteur ligne\n",
    "y2 = np.dot(x2, Ro)\n",
    "\n",
    "print('Matrice de Rotation')\n",
    "print(Ro)\n",
    "print('\\nVecteur rotatif')\n",
    "print(y2)\n",
    "\n",
    "print('\\n nome x2', np.linalg.norm(x2))\n",
    "print('\\n norme y2', np.linalg.norm(y2))\n",
    "print('\\n norme de la matrice de Roation', np.linalg.norm(Ro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMOSBn8YsoRG"
   },
   "source": [
    "* La norme du vecteur d'entrée est la même que la norme du vecteur de sortie. Les matrices de rotation ne modifient pas la norme du vecteur, mais seulement sa direction.\n",
    "* La norme de toute matrice de rotation de $R^2$ est toujours $\\sqrt 2 = 1.414221$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "lQ56WwLssoRH",
    "outputId": "f4f80e00-6ad9-4656-a0f3-5996cfb00926"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPLElEQVR4nO3dfWxd9XnA8e9jh7yQEoGEIZDAiASCAYLBTCa0kbFAtyxhA4YQkLEROjVZog5agboW0FA3VWxC0EpbUBoJmrZCoEjUYuqCCmEggjpeHEhZXiBCqEugYbiLGLXCS0ye/eGbLIBJfH2Pfe7P/n4kK7n25Zzn4PDll+Nz74nMRJJUro66B5AktcaQS1LhDLkkFc6QS1LhDLkkFc6QS1LhWg55REyNiBci4ucRsSUivlXFYJKk4YlWryOPiACmZ2Z/RBwBPAvcnJnPVTGgJOnQJrW6gRz8P0F/4+ERjQ9fZSRJY6TlkANERCewETgVWJmZzw/xnKXAUoDp06f/9hlnnFHFriVpwti4ceOvMrPr059v+dTKJzYWcTTQA/xNZm7+vOd1d3dnb29vZfuVpIkgIjZmZvenP1/pVSuZ+S7wFLCgyu1Kkj5fFVetdDVW4kTENOCLwKutbleSNDxVnCM/AfhB4zx5B7A2M39SwXYlScNQxVUrrwDnVTCLJGkEfGWnJBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBXOkEtS4Qy5JBWu5ZBHxEkR8VREbI2ILRFxcxWDSZKGZ1IF2xgAbsnMlyLiKGBjRDyRmVsr2LYk6TBaXpFn5q7MfKnx+18D24BZrW5XkjQ8lZ4jj4hTgPOA56vcriTp81UW8oj4AvAI8NXMfG+Iry+NiN6I6O3r66tqt5I04VUS8og4gsGIP5iZPx7qOZm5OjO7M7O7q6urit1KkqjmqpUA7ge2Zea9rY8kSWpGFSvy3wX+ApgfEZsaHwsr2K4kaRhavvwwM58FooJZJEkj4Cs7JalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCjep7gEkjZ63336b3t5ePvzwQ6666qq6x9EoMeTSONHX18fGjRt54YUXePrpp9m0aRN79uyho6ODyZMnc+WVV9LR4V/Cx6NKQh4RDwCXAe9k5tlVbFPS8M2fP58NGzZw5JFHsmfPHgYGBg58rbOzkyVLlhjxcayq7+waYEFF25LUpGXLljFlyhT6+/s/EXGAKVOmsGzZspom01ioJOSZ+Qywu4ptSWreNddcwyuvvEJEfOZrM2fO5Nxzz61hKo2VMfu7VkQsjYjeiOjt6+sbq91KE0Jmcs011/Dxxx8zadIkpk2bBsC0adNYsWJFzdNptI1ZyDNzdWZ2Z2Z3V1fXWO1WGvd27txJR0cHvb299PT0sHfvXtauXcuMGTMYGBjg+uuvr3tEjTKvWpEKtmrVKpYvXw7A7t27OeaYYwC47LLL2LJlC1u3buX444+vc0SNAUMuFWjfvn3MmTOHHTt2cPXVV7N27drPPGf27NnMnj27huk01io5tRIRDwH/AZweEW9GxF9VsV1Jn7V9+3Y6OzvZsWMH69evHzLimlgqWZFn5nVVbEfSod11113cdtttAPT39zN9+vSaJ1I78NSKVIC9e/cyY8YMPvjgA1asWMHKlSvrHkltxJBLbe7ll1/m/PPPB+D5559n7ty5NU+kduNrdqU2duuttx6I+AcffGDENSRDLrWh999/n4jgnnvu4Y477iAzmTJlSt1jqU15akVqM88++ywXXXQRAJs3b+ass86qeSK1O1fkUhu58cYbueiii+jq6mJgYMCIa1gMudQG3nvvPSKCNWvW8J3vfId33nmHzs7OusdSITy1ItXsscceY+HChQC88cYbzJkzp+aJVBpX5FJNMpNFixaxcOFCzjrrrAMvu5ea5YpcqkFfXx/HHXccAGvWrOGGG26oeSKVzJBLY+yhhx5i8eLFAOzatYuZM2fWPJFK56kVaYxkJhdccAGLFy/m4osvZt++fUZclXBFLo2BnTt3cvLJJwPQ09PDFVdcUe9AGldckUujbNWqVQcivnv3biOuyrkil0bJcG7+IFXBkEujYPv27Zx++ukAPPnkk8yfP7/miTSeeWpFqthdd911IOL9/f1GXKPOFblUkYNv/rB8+XLuu+++ukfSBGHIpQp48wfVyVMrUotuueUWb/6gWhlyaYT23/zh3nvv9eYPqpWnVqQR2LBhA/PmzQO8+YPq54pcatKNN97IvHnzOPbYY735g9qCIZeG6dM3f+jr6/PmD2oLnlqRhmHdunUsWrQI8OYPaj+uyKVD2H/zh0WLFnnzB7UtV+TS5/DmD4N+9jPo7IS5cyGi7mk0lEpW5BGxICJei4jXI+IbVWxTqtPDDz98IOK7du2asBEHuP56mDcPzjgDvv99eP/9uifSp7Uc8ojoBFYCfwycCVwXEWe2ul2pDvtv/nDdddd584eGTPjoI9i+HW66CY47Dr72NfjFL+qeTPtVcWplLvB6Zr4BEBEPA5cDWyvYtjRmMqGj47vAD5k1azZvv30UZ7ok+USw+/sHf125ElatggsvhB/9CGbNqmU0NVQR8lnAzoMevwn8zqefFBFLgaXAgTfZl9rJRx/BiSf+Cb/85am89Vbd07S3vXth8uTBVfqePXVPozG7aiUzV2dmd2Z2d3V1jdVupWGbMgXeeutUMvHjoI9TTvn/f0eTJ8PUqXDJJdDTAzt2wGmn1fYtU0MVK/K3gJMOejy78TlJ48TUqTBpEnz5y4PnyQ+Ou+pXRchfBE6LiDkMBvxaYHEF25XUBu68c/DXa68dDLraT8shz8yBiPgK8FOgE3ggM7e0PJmktrBkSd0T6HAqeUFQZq4D1lWxLUlSc3yJviQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVzpBLUuEMuSQVrqWQR8TVEbElIvZFRHdVQ0mShq/VFflm4M+AZyqYRZI0ApNa+YczcxtARFQzjSSpaWN2jjwilkZEb0T09vX1jdVuJWncO+yKPCLWAzOH+NLtmfnocHeUmauB1QDd3d057AklSYd02JBn5qVjMYgkaWS8/FCSCtfq5YdXRsSbwIXAv0XET6sZS5I0XK1etdID9FQ0iyRpBDy1IkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFM+SSVDhDLkmFaynkEXF3RLwaEa9ERE9EHF3RXJKkYWp1Rf4EcHZmngNsB77Z+kiSpGa0FPLMfDwzBxoPnwNmtz6SJKkZVZ4j/xLwWIXbkyQNw6TDPSEi1gMzh/jS7Zn5aOM5twMDwIOH2M5SYCnAySefPKJhJUmfddiQZ+alh/p6RCwBLgMuycw8xHZWA6sBuru7P/d5kqTmHDbkhxIRC4CvA7+fmXuqGUmS1IxWz5H/C3AU8EREbIqIVRXMJElqQksr8sw8tapBJEkj4ys7JalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCmfIJalwhlySCtdSyCPiHyLilYjYFBGPR8SJVQ0mSRqeVlfkd2fmOZn5W8BPgL9rfSRJUjNaCnlmvnfQw+lAtjaOJKlZk1rdQER8G/hL4H+BPzjE85YCSxsP+yPitVb3fZBjgV9VuL06eAztwWNoDx7D0H5jqE9G5qEX0RGxHpg5xJduz8xHD3reN4GpmXlnK1OORET0Zmb3WO+3Sh5De/AY2oPH0JzDrsgz89JhbutBYB0w5iGXpIms1atWTjvo4eXAq62NI0lqVqvnyP8xIk4H9gH/Bfx16yONyOqa9lslj6E9eAztwWNowmHPkUuS2puv7JSkwhlySSrcuAn5eHi7gIi4OyJebRxHT0QcXfdMzYqIqyNiS0Tsi4iiLh+LiAUR8VpEvB4R36h7nmZFxAMR8U5EbK57lpGKiJMi4qmI2Nr4c3Rz3TM1KyKmRsQLEfHzxjF8a9T3OV7OkUfEjP2vNI2Im4AzM7OuH76OSET8IfDvmTkQEf8EkJl/W/NYTYmI32Twh9/fA27NzN6aRxqWiOgEtgNfBN4EXgSuy8yttQ7WhIiYB/QDP8zMs+ueZyQi4gTghMx8KSKOAjYCVxT2fQhgemb2R8QRwLPAzZn53Gjtc9ysyMfD2wVk5uOZOdB4+Bwwu855RiIzt2Vmla/aHStzgdcz843M/Ah4mMFLaouRmc8Au+ueoxWZuSszX2r8/tfANmBWvVM1Jwf1Nx4e0fgY1R6Nm5DD4NsFRMRO4M8p/w28vgQ8VvcQE8gsYOdBj9+ksICMNxFxCnAe8HzNozQtIjojYhPwDvBEZo7qMRQV8ohYHxGbh/i4HCAzb8/Mkxh8lelX6p12aIc7hsZzbgcGGDyOtjOcY5BaERFfAB4Bvvqpv20XITM/brwr7GxgbkSM6qmult80ayyNh7cLONwxRMQS4DLgkmzTH2A08X0oyVvASQc9nt34nMZY47zyI8CDmfnjuudpRWa+GxFPAQuAUfshdFEr8kMZD28XEBELgK8Df5qZe+qeZ4J5ETgtIuZExGTgWuBfa55pwmn8oPB+YFtm3lv3PCMREV37rziLiGkM/gB9VHs0nq5aeQT4xNsFZGZRK6qIeB2YAvxP41PPFXjlzZXAPwNdwLvApsz8o1qHGqaIWAh8F+gEHsjMb9c7UXMi4iHgYgbfPvW/gTsz8/5ah2pSRPwesAH4Twb/Wwa4LTPX1TdVcyLiHOAHDP456gDWZubfj+o+x0vIJWmiGjenViRpojLkklQ4Qy5JhTPkklQ4Qy5JhTPkklQ4Qy5Jhfs/LeSFVtbN5rMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_vectors([x2, y2], fname='transform_02.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hSg8WLvsoRH"
   },
   "source": [
    "Deux vecteurs formant un angle de 45°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDvG39MDsoRH"
   },
   "source": [
    "Dans notre cas nous voulons un angle de 90° pour tracer la ligne(l'hyperplan) qui sépare nos deux plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "CvFwUJM3soRI",
    "outputId": "48eb5a46-bf1f-4c13-841c-15d87c71ded6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "P = np.array([[1, 1]])  \n",
    "\n",
    "# Obtenir un nouveau plan perpendiculaire à P. Nous utilisons une matrice de rotation\n",
    "PT = np.dot([[0, 1], [-1, 0]], P.T).T  \n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8)) #la taille de la figure\n",
    "\n",
    "plot_vectors([P], colors=['b'], axes=[2, 2], ax=ax1) # Tracez le plan P comme un vecteur\n",
    "\n",
    "# Tracez le plan P comme 2 vecteurs. \n",
    "# On met à l'échelle par 2 juste pour que les flèches sortent de la boîte\n",
    "plot_vectors([PT * 4, PT * -4], colors=['k', 'k'], axes=[4, 4], ax=ax1)\n",
    "\n",
    "# Plot 20 random points. \n",
    "for i in range(0, 20):\n",
    "        v1 = np.array(np.random.uniform(-4, 4, 2)) # Obtenez une paire de nombres aléatoires entre -4 et 4 \n",
    "        side_of_plane = np.sign(np.dot(P, v1.T)) # Obtenir le signe du produit scalaire avec P\n",
    "        # Colorer les points en fonction du signe du résultat de np.dot(P, point.T)\n",
    "        if side_of_plane == 1:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'bo') #  un point bleu\n",
    "        else:\n",
    "            ax1.plot([v1[0]], [v1[1]], 'ro') # un point rouge\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXpIqH_JsoRI"
   },
   "source": [
    "Maintenant si on choisis 3 points $v_1=[1,2],v_2=[-1,1]$ et $v_3=[-2,-1]$, comment savoir de quel coté du plan se trouve chaque point ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds9_zYzCsoRI"
   },
   "source": [
    "<img src=\"images/lsh1.png\" style=\"width:700px;height:400;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGlkVd-hsoRJ"
   },
   "source": [
    "Pour celà nous devons faire le produit scalaire de P et de chaque point on a : \n",
    "- $Pv_1^{T}=3$\n",
    "- $Pv_2^{T}=0$\n",
    "- $Pv_3^{T}=-3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Ggl7_s1WsoRJ"
   },
   "outputs": [],
   "source": [
    "P = np.array([[1, 1]])      # plan unique\n",
    "v1 = np.array([[1, 2]])     \n",
    "v2 = np.array([[-1, 1]])    \n",
    "v3 = np.array([[-2, -1]])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "-Aex9aMTsoRK",
    "outputId": "3ab3e5ef-ac76-424f-a133-c2f654c61615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(P, v1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ffgL6fePsoRK",
    "outputId": "f2812e06-3697-471f-8086-82bb481d8e5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(P, v2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "WszRQUoqsoRL",
    "outputId": "467bc433-9ad2-4d3f-ed4b-d725ed3b1de6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(P, v3.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TULoQhANsoRM"
   },
   "source": [
    "Comme on l'a vu précédement avec l'Hyperplan:\n",
    "\n",
    "- Si le signe du produit scalaire entre P et un point v est positif alors ce point se trouve dans le plan de droite (points bleus ), la valeur de hashage vaudra 1, **1 car il se trouve du meme coté que le vecteur normal P.**\n",
    "- Si le signe du produit scalaire entre P et un point v est positif et négatif alors ce point se trouve sur la droite perpendiculaie au vecteur P,  la valeur de hashage vaudra 0, **il se trouve sur l'hyperplan.**\n",
    "- Si le signe du produit scalaire entre P et un point v est négatif alors ce point se trouve dans le plan de gauche (points rouges ), la valeur de hashage vaudra -1, **-1 car il se trouve du coté opposé au vecteur normal P.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkRx4qtosoRM"
   },
   "source": [
    "<a name='slide'></a>\n",
    "La fonction `side_of_plane()` ci-dessous vérifie de quel côté du plan P se trouve le vecteur `v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "0-kU_lqOsoRN"
   },
   "outputs": [],
   "source": [
    "def side_of_plane(P, v):\n",
    "    dotproduct = np.dot(P, v.T) # Get the dot product P * v'\n",
    "    sign_of_dot_product = np.sign(dotproduct) # The sign of the elements of the dotproduct matrix \n",
    "    sign_of_dot_product_scalar = sign_of_dot_product.item() # The value of the first item\n",
    "    return sign_of_dot_product_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "If6GlpBOsoRN",
    "outputId": "c58268a0-4d6e-445c-c26f-6c9d2d622067"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "side_of_plane(P, v1) # Dans quel coté se trouve [1, 2] ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "VqbirPZksoRO",
    "outputId": "4cc51d81-9a1d-47b5-d000-b9d9194534f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "side_of_plane(P, v2) # et [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "npfqSbFasoRP",
    "outputId": "01b5a038-0db7-42d7-a945-9b444b474a6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "side_of_plane(P, v3) # et [-2, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDY7Unh1soRP"
   },
   "source": [
    "## <a name=\"vhpp\">Valeur de hachage avec plusieurs plans</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkXt93vXsoRQ"
   },
   "source": [
    "Lorsque l'on a plusieurs plan comment trouver le coté d'un point v pour chacun des plans $P_1, P_2$ et $P_3$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCQ2flbrsoRQ"
   },
   "source": [
    "<img src=\"images/multip.png\" style=\"width:700px;height:400;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z0W6IqksoRR"
   },
   "source": [
    "On considère 3 plans définit par $P_1=[1,1], P_2=[-1,1]$ et $P_3=[-1,-1]$ et le vecteur $v=[2,2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXpaEwhksoRR"
   },
   "source": [
    "On souhaiterais avoir une seul valeur de hachage pour les combiner\n",
    "\n",
    "Pour chaque plan $P_i$ et le vecteur $v$ \n",
    "\n",
    "on définit $sign_i$ tel que :\n",
    "- $sign_i=sign(P_iv^{T}) $\n",
    "\n",
    "et $h_i$ tel que : \n",
    "- $sign_i \\geq 0 \\Rightarrow h_i=1$\n",
    "- $sign_i < 0 \\Rightarrow h_i=0$\n",
    "\n",
    "On définit alors la valeur de hachage sensible à la localité par : \n",
    "$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$ avec N le nombre de plans\n",
    "\n",
    "Dans notre cas: \n",
    " \n",
    "- $P_1v^t=3, sign_1=+1, h_1=1$\n",
    "- $P_2v^t=5, sign_2=+1, h_2=1$\n",
    "- $P_3v^t=-2, sign_3=-1, h_3=0$\n",
    "- $hash = 2^0h_1+2^1h_2+2^2h_3$= 1x1+2x1+4x0=3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmGkOgYasoRS"
   },
   "source": [
    "Implémentons celà avec la fonction suivante crée une valeur de hachage basée sur un ensemble de plans. **La valeur de sortie est une combinaison du côté du plan où le vecteur est localisé par rapport à l'ensemble des plans.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Gq82DKMosoRS"
   },
   "outputs": [],
   "source": [
    "P1 = np.array([[1, 1]])   # Premier plan\n",
    "P2 = np.array([[-1, 1]])  \n",
    "P3 = np.array([[-1, -1]]) \n",
    "P_l = [P1, P2, P3]  # Liste de vecteur. c'est le multiplan\n",
    "\n",
    "# Vecteur à rechercher\n",
    "v = np.array([[2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "WrT-vhc6soRS",
    "outputId": "511d980c-a885-48e7-856e-f285ad9a8e42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1, 1]]), array([[-1,  1]]), array([[-1, -1]])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFQr0GyWsoRT"
   },
   "source": [
    "<a name='hsmp'></a>\n",
    "La fonction `hash_multi_plane()`suivante crée une valeur de hachage basée sur un ensemble de plans. La valeur de sortie est une combinaison du côté du plan où le vecteur est localisé par rapport à l'ensemble des plans.\n",
    "\n",
    "Nous pouvons considérer cette liste de plans comme un ensemble de fonctions de hachage de base, chacune d'entre elles ne pouvant produire que 1 ou 0 en sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "dTPnfpilsoRT"
   },
   "outputs": [],
   "source": [
    "def hash_multi_plane(P_l, v):\n",
    "    hash_value = 0\n",
    "    for i, P in enumerate(P_l): # i:P , 0:[[1 1]], 1:[[-1  1]],2:[[-1 -1]]\n",
    "        sign = side_of_plane(P,v)  # on utilise notre fonction précédente\n",
    "        hash_i = 1 if sign >=0 else 0\n",
    "        hash_value += 2**i * hash_i\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "pCX1xihAsoRT",
    "outputId": "bcf07983-dd2b-4a93-fb69-ce6d9e267d24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_multi_plane(P_l, v) # Trouvez le numéro du plan qui contient cette valeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zR5cGJncsoRT"
   },
   "source": [
    "## <a name=\"pa\">Plans aléatoires</a>\n",
    "\n",
    "\n",
    "Dans la cellule ci-dessous, nous créons un ensemble de trois plans aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "wjrhf46LsoRU",
    "outputId": "3aebede9-d99e-4754-d6f8-86be75ca851f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721]\n",
      " [ 0.97873798  2.2408932 ]\n",
      " [ 1.86755799 -0.97727788]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_dimensions = 2 # is 300 in assignment\n",
    "num_planes = 3 # is 10 in assignment\n",
    "random_planes_matrix = np.random.normal(  # loi normal \n",
    "                       size=(num_planes,\n",
    "                             num_dimensions))\n",
    "print(random_planes_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CP-n2WdSsoRU"
   },
   "source": [
    "On choisit un vecteur v=[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "nTW1whGisoRU"
   },
   "outputs": [],
   "source": [
    "v = np.array([[2, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJJY7nINsoRV"
   },
   "source": [
    "La fonction suivante est similaire à la fonction [`side_of_plane()`](#slide), mais elle évalue plus qu'un plan à chaque fois. Le résultat est un tableau avec le côté du plan de `v`, pour l'ensemble des plans `P`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "N1n5phmjsoRW"
   },
   "outputs": [],
   "source": [
    "# Side of the plane function. The result is a matrix\n",
    "def side_of_plane_matrix(P, v):\n",
    "    dotproduct = np.dot(P, v.T)\n",
    "    sign_of_dot_product = np.sign(dotproduct) # Get a boolean value telling if the value in the cell is positive or negative\n",
    "    return sign_of_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AUP8iZ8soRW"
   },
   "source": [
    "Obtenir le côté du plan du vecteur `[2, 2]` pour l'ensemble des plans aléatoires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "AD9KqdJcsoRW",
    "outputId": "e0091415-1daf-4fcf-f033-3a9aaeb4b6cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sides_l = side_of_plane_matrix(\n",
    "            random_planes_matrix, v)\n",
    "sides_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tibmqPQWsoRX"
   },
   "source": [
    "coté 1 de chaque plan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOp9drPGsoRX"
   },
   "source": [
    "Maintenant, utilisons la première fonction  [`hash_multi_plane()`](#hsmp) pour définir notre fonction `hash_multi_plane_matrix()` de hachage multiplan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "0KPTqgVysoRX"
   },
   "outputs": [],
   "source": [
    "def hash_multi_plane_matrix(P, v, num_planes):\n",
    "    sides_matrix = side_of_plane_matrix(P, v) # Obtenir le côté des plans pour P et v\n",
    "    hash_value = 0\n",
    "    for i in range(num_planes):\n",
    "        sign = sides_matrix[i].item() # Obtenir la valeur à l'intérieur de la cellule de la matrice\n",
    "        hash_i = 1 if sign >=0 else 0\n",
    "        hash_value += 2**i * hash_i # somme de 2^i * hash_i\n",
    "        \n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "688L-clisoRY"
   },
   "source": [
    "Affichage du seau de hachage pour le vecteur `v = [2, 2]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "34Frt8MPsoRY",
    "outputId": "f8516113-1d4f-4f86-b791-558282984339"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_multi_plane_matrix(random_planes_matrix, v, num_planes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm6CffRBsoRZ"
   },
   "source": [
    "\n",
    "Cela vous a montré comment faire une série de plan  au hasard.  Nous ferons plusieurs séries d'avions aléatoires afin de rendre les voisins les plus proches plus précis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHo0w3fbsoRZ"
   },
   "source": [
    "## <a name=\"ttpsh\">Trouver les tweets les plus similaires avec le HSL</a>\n",
    "\n",
    "On va maintenant mettre en œuvre le hachage sensible à la localité (HSL) pour identifier le [tweet](#idt) le plus similaire.\n",
    "* Au lieu de regarder les 10 000 vecteurs, nous pouvons simplement rechercher un sous-ensemble pour trouver\n",
    "ses voisins les plus proches.\n",
    "\n",
    "Disons que vos points de données sont tracés comme ceci :\n",
    "\n",
    "\n",
    "<div style=\"width:image width px ; font-size:100% ; text-align:center ;\"><img src='images/one.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px ;\" /> </div>\n",
    "\n",
    "Nous pouvons diviser l'espace vectoriel en régions et rechercher dans une région les voisins les plus proches d'un vecteur donné.\n",
    "\n",
    "<div style=\"width:image width px ; font-size:100% ; text-align:center ;\"><img src='images/four.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px ;\" /> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "tsZS9tY2soRZ",
    "outputId": "3a16337a-6fae-43a3-8f2d-b0d7c955c258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de vecteurs est de 10000 et chacun a une dimensions de 300 .\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_tweets)       \n",
    "N_DIMS = len(ind2Tweet[1])  \n",
    "print(f\"Le nombre de vecteurs est de {N_VECS} et chacun a une dimensions de {N_DIMS} .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koPpV2uNsoRa"
   },
   "source": [
    "### <a name=\"cnp\">Choisir le nombre de plans</a>\n",
    "\n",
    "* Chaque plan divise l'espace en 2 seaux .\n",
    "* Donc les $n$ plans  divisent l'espace en seaux de hachage à $2^{n} parties$.\n",
    "* Nous voulons organiser 10 000 vecteurs de documents (tweets) dans des seaux, de sorte que chaque seaux contiennent environ 16 vecteurs.\n",
    "* Pour cela, nous avons besoin de $\\frac{10000}{16}=625$ seaux.\n",
    "* Nous sommes intéressés par $n$, nombre de plans, donc $2^{n}=625$. Maintenant, nous pouvons calculer $n=\\log_{2}625 = 9,29 \\approx 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "LAe5AXcnsoRa"
   },
   "outputs": [],
   "source": [
    "# Le nombre de plans. Nous utilisons log2(625) pour avoir ~16 vecteurs/seau.\n",
    "N_PLANES = 10\n",
    "# Nombre de fois où il faut répéter le hachage pour améliorer la recherche.\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHgM7q2bsoRa"
   },
   "source": [
    "### <a name=\"ovhv\">Obtenir la valeur de hachage d'un vecteur</a>\n",
    "\n",
    "Pour chaque vecteur, nous devons obtenir un numéro unique associé à ce vecteur afin de l'attribuer à un \"seau de hachage\".\n",
    "\n",
    " On va créer des séries de plans aléatoires:`planes_l` \n",
    "* Créons  plusieurs ensembles(25) de plans (les plans qui divisent la région).\n",
    "* Nous pouvons considérer qu'il y a 25 façons différentes de diviser l'espace vectoriel avec un ensemble de plans différent.\n",
    "* Chaque élément de cette liste contient une matrice de 300 lignes ( dimensions de 300 ) et 10 colonnes (il y a 10 plans dans chaque \"univers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "5beNbCXlsoRb"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCAL61QosoRb"
   },
   "source": [
    "On va implémenter la fonction `hash_value_of_vector` qui contrairement à [`hash_multi_plane()`](#hsmp) définit h comme un vecteur remplit de 0 ou 1 et qui place le vecteur `v` dans le bon seau de hachage .\n",
    "\n",
    "* On multipliera d'abord notre vecteur `v`, par un plan correspondant. Cela vous donnera un vecteur de dimension $(1,\\text{N_plans})$.\n",
    "* On convertira ensuite chaque élément de ce vecteur en 0 ou 1.\n",
    "* Puis nous créerons un vecteur de hachage h en faisant ce qui suit : si l'élément $h[i]$ est négatif, il devient un 0, sinon nous le changeons en un 1.\n",
    "* On calcule ensuite le nombre unique (valeur de hachage) pour le vecteur par itération sur `N_PLANES`.\n",
    "* Ensuite, on multiolie $2^i$ par le $h[i]$ correspondant (0 ou 1).\n",
    "* On va  ensuite stocker cette somme dans la variable `hash_value`.\n",
    "\n",
    ".\n",
    "On utilisera cette formule :\n",
    "\n",
    "$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "V3wUHsUSsoRb"
   },
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Créer un hash pour un vecteur ; hash_id indique quel hash aléatoire utiliser.\n",
    "    Entrée :\n",
    "        - v : vecteur de tweet. Sa dimension est (1, N_DIMS)\n",
    "        - planes : matrice de dimension (N_DIMS, N_PLANES) - l'ensemble des plans qui divisent la région\n",
    "    Sortie :\n",
    "        - hash_value : un numéro qui est utilisé comme hachage pour votre vecteur\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    # pour la série de plans,\n",
    "    # calculer le produit du point entre le vecteur et la matrice contenant les plans\n",
    "    # se rappeler que les plans ont une taille (300, 10)\n",
    "    # Le produit scalaire aura une taille (1,10)\n",
    "    dot_product = np.dot(v,planes)\n",
    "\n",
    "    # obtenir le signe du produit scalaire\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "\n",
    "    # mettre h à faux (équivalent à 0 lorsqu'il est utilisé dans les opérations) si le signe est négatif,\n",
    "    # et vrai (équivalent à 1) si le signe est positif \n",
    "    h = sign_of_dot_product>=0\n",
    "\n",
    "    # supprimer les dimensions inutilisées supplémentaires (convertir ce tableau de 2D en tableau de 1D)\n",
    "    h = np.squeeze(h)\n",
    "\n",
    "   # initialiser la valeur de hachage à 0\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        # augmenter la valeur de hachage de 2^i * h_i\n",
    "        hash_value += np.power(2,i)*h[i]\n",
    "\n",
    "    # utiliser hash_value comme un entier\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "4wjikVpqsoRc",
    "outputId": "509c0607-f3b6-4760-da90-461923bdd9b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La valeur de hachage pour ce vecteur, et l'ensemble des plans à l'index 0, est 768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "idx = 0\n",
    "planes = planes_l[idx]  # obtenir un \"univers\" de plan pour tester la fonction\n",
    "vec = np.random.rand(1, 300)\n",
    "print(f\" La valeur de hachage pour ce vecteur,\",\n",
    "      f\"et l'ensemble des plans à l'index {idx},\",\n",
    "      f\"est {hash_value_of_vector(vec, planes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz46BJKBsoRc"
   },
   "source": [
    "### <a name=\"cth\">Création d'une table de hachage</a>\n",
    "\n",
    "Étant donné que nous avons un numéro unique pour chaque vecteur (ou tweet), nous voulons maintenant créer une table de hachage. Nous avons besoin d'une table de hachage, de sorte que, étant donné un hash_id, nous puissions rapidement rechercher les vecteurs correspondants. Cela nous permet de réduire considérablement le temps de recherche.\n",
    "\n",
    "<div style=\"width:image width px ; font-size:100% ; text-align:center ;\"><img src='images/table.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:500px;height:200px ;\" /> </div>\n",
    "\n",
    "Nous avons la fonction `make_hash_table`, qui fait correspondre les vecteurs de tweet à un seau et y stocke le vecteur. Elle renvoie la `hash_table` et la `id_table`. L'\"id_table\" nous permet de savoir quel vecteur dans une certaine zone correspond à quel tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "kTclAq94soRc"
   },
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes):\n",
    "    \"\"\"\n",
    "    Entrée :\n",
    "        - vecs : liste des vecteurs à hacher.\n",
    "        - planes : la matrice des plans d'un \"univers\" unique, avec la forme (dimensions d'embeddings, nombre de plans).\n",
    "    Sortie :\n",
    "        - hash_table : dictionnaire - les clés sont des hachages, les valeurs sont des listes de vecteurs (seau de hachage)\n",
    "        - id_table : dictionnaire - les clés sont des hachages, les valeurs sont des listes de vecteurs id\n",
    "                            (il permet de savoir quel tweet correspond au vecteur haché)\n",
    "    \"\"\"\n",
    "\n",
    "    # le nombre de plans est le nombre de colonnes dans la matrice des plans\n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    # le nombre de seaux est de 2^(nombre d'avions)\n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    # créer la table de hachage comme un dictionnaire.\n",
    "    # Les clés sont des entiers (0,1,2.. nombre de seaux)\n",
    "    # Les valeurs sont des listes vides\n",
    "    hash_table = {i :[] for i in range(num_buckets)}\n",
    "\n",
    "    # créer la table d'identification comme un dictionnaire.\n",
    "    # Les clés sont des entiers (0,1,2... nombre de seaux)\n",
    "    # Les valeurs sont des listes vides\n",
    "    id_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    # pour chaque vecteur dans les \"vecs\".\n",
    "    for i, v in enumerate(vecs):\n",
    "        # calculer la valeur de hachage pour le vecteur\n",
    "        h = hash_value_of_vector(v,planes)\n",
    "\n",
    "        # stocker le vecteur dans hash_table à la clé h,\n",
    "        # en ajoutant le vecteur v à la liste à la touche h\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        # stocker l'index 'i' du vecteur (chaque document reçoit un entier unique 0,1,2...)\n",
    "        # la clé est le h, et le 'i' est ajouté à la liste à la clé h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "9zCt9SvFsoRd",
    "outputId": "3f63b3c7-f859-47e6-a30d-bd5a51f3ec68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La table de hachage à la clé 0 a 3 vecteurs de documents\n",
      "La table d'identification à la clé 0 a 3\n",
      "Les 5 premiers index de documents stockés à la clé 0  sont [3276, 3281, 3282]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "planes = planes_l[0]  # obtenir un \"univers\" de plans pour tester la fonction\n",
    "vec = np.random.rand(1, 300)\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
    "\n",
    "print(f\"La table de hachage à la clé 0 a {len(tmp_hash_table[0])} vecteurs de documents\")\n",
    "print(f\"La table d'identification à la clé 0 a {len(tmp_id_table[0])}\")\n",
    "print(f\"Les 5 premiers index de documents stockés à la clé 0  sont {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj45VZAFsoRd"
   },
   "source": [
    "### <a name=\"ctth\">Création de toutes les tables de hachage</a>\n",
    "\n",
    "Nous pouvons maintenant hacher nos vecteurs et les stocker dans une table de hachage qui\n",
    "nous permettrait de rechercher rapidement des vecteurs similaires.\n",
    "Lançons la cellule ci-dessous pour créer les hachages. En faisant cela, nous finissons par avoir\n",
    "plusieurs tables qui ont tous les vecteurs. Étant donné un vecteur, vous\n",
    "identifier les seaux de toutes les tables.  Nous pouvons ensuite itérer sur le\n",
    "seaux et envisager beaucoup moins de vecteurs. Plus nous utilisons de seaux, plus notre recherche sera précise, mais aussi plus elle prendra de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "EsdyLQG_soRd",
    "outputId": "6a3a121b-bd14-4a7a-9341-f43ca2bad5d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travaillant sur le hash universe # : 0\n",
      "travaillant sur le hash universe # : 1\n",
      "travaillant sur le hash universe # : 2\n",
      "travaillant sur le hash universe # : 3\n",
      "travaillant sur le hash universe # : 4\n",
      "travaillant sur le hash universe # : 5\n",
      "travaillant sur le hash universe # : 6\n",
      "travaillant sur le hash universe # : 7\n",
      "travaillant sur le hash universe # : 8\n",
      "travaillant sur le hash universe # : 9\n",
      "travaillant sur le hash universe # : 10\n",
      "travaillant sur le hash universe # : 11\n",
      "travaillant sur le hash universe # : 12\n",
      "travaillant sur le hash universe # : 13\n",
      "travaillant sur le hash universe # : 14\n",
      "travaillant sur le hash universe # : 15\n",
      "travaillant sur le hash universe # : 16\n",
      "travaillant sur le hash universe # : 17\n",
      "travaillant sur le hash universe # : 18\n",
      "travaillant sur le hash universe # : 19\n",
      "travaillant sur le hash universe # : 20\n",
      "travaillant sur le hash universe # : 21\n",
      "travaillant sur le hash universe # : 22\n",
      "travaillant sur le hash universe # : 23\n",
      "travaillant sur le hash universe # : 24\n"
     ]
    }
   ],
   "source": [
    "# Creating the hashtables\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):  # il y a 25 hachages\n",
    "    print('travaillant sur le hash universe # :', universe_id)\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybtPbw_EeFDJ"
   },
   "source": [
    "### <a name=\"akuh\">Avec KNN en utilisant HSL</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Py7isu8qsoRe"
   },
   "source": [
    "Mettons en place les K plus proches voisins en utilisant le hachage sensible à la localité,\n",
    "pour rechercher des documents similaires (ici nos tweets) à un document donné à\n",
    "l'index `doc_id`.\n",
    "\n",
    "Entrées:\n",
    "* `doc_id` est l'index dans la liste des documents `all_tweets`.\n",
    "* `v` est le vecteur de document pour le tweet dans `all_tweets` à l'index `doc_id`.\n",
    "* `planes_l` est la liste des plans (la variable globale créée précédemment).\n",
    "* `k` est le nombre de voisins les plus proches à rechercher.\n",
    "* `num_universes_to_use` : pour gagner du temps, on peut utiliser moins que le total\n",
    "nombre d'univers disponibles.  Par défaut, il est fixé à `N_UNIVERSES`,\n",
    "qui est de $25$ ici.\n",
    "\n",
    "La fonction `approximate_knn` trouve un sous-ensemble de vecteurs candidats qui\n",
    "se trouvent dans le même \"seau de hachage\" que le vecteur d'entrée \"v\".  Ensuite, il effectue\n",
    "les k plus proches voisins qui recherchent habituellement sur ce sous-ensemble (au lieu de rechercher\n",
    "à travers les 10 000 tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "Ff85lD6esoRe"
   },
   "outputs": [],
   "source": [
    "def approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n",
    "    \"\"\"Recherche de k-NN à l'aide de hachages.\"\"\"\n",
    "    assert num_universes_to_use <= N_UNIVERSES\n",
    "\n",
    "    # Vecteurs qui seront contrôlés en tant que voisin le plus proche possible\n",
    "    vecs_to_consider_l = list()\n",
    "\n",
    "    # liste des documents d'identité\n",
    "    ids_to_consider_l = list()\n",
    "\n",
    "    # créer un ensemble d'identifiants à prendre en compte, pour vérifier plus rapidement si un identifiant de document existe déjà dans l'ensemble\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    # boucle à travers les univers des plans\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "\n",
    "        # obtenir l'ensemble des plans de la liste planes_l, pour cet universe_id particulier\n",
    "        planes = planes_l[universe_id]\n",
    "\n",
    "         # obtenir la valeur de hachage du vecteur pour cet ensemble dde plans \n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # obtenir la table de hachage pour cet univers_id particulier\n",
    "        hash_table = hash_tables[universe_id]\n",
    "\n",
    "        # obtenir la liste des vecteurs de documents pour cette table de hachage, où la clé est la hash_value\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "\n",
    "        # obtenir l'id_table pour cet univers_id particulier\n",
    "        id_table = id_tables[universe_id]\n",
    "\n",
    "        # obtenir le sous-ensemble de documents à considérer comme les plus proches voisins à partir de ce dictionnaire id_table\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "         # supprimer l'identifiant du document que nous recherchons\n",
    "        if doc_id in new_ids_to_consider:\n",
    "            new_ids_to_consider.remove(doc_id)\n",
    "            print(f\"suppréssion du doc_id {doc_id} du vecteur d'entrée de new_ids_to_search\")\n",
    "\n",
    "         # boucler sur le sous-ensemble des vecteurs de documents à prendre en compte\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "\n",
    "             # si l'ID du document n'est pas encore dans l'ensemble ids_to_consider...\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                 # accéder à la liste document_vectors_l à l'index i pour obtenir l'intégration\n",
    "                # puis l'ajouter à la liste des vecteurs à considérer comme de possibles voisins proches\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "\n",
    "                # ajouter le new_id (l'index du document) à la liste des ids à prendre en compte\n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "                ids_to_consider_l.append(new_id)\n",
    "\n",
    "                 # ajoutez également le new_id à l'ensemble des ids à prendre en compte\n",
    "                # (utilisez ceci pour vérifier si le new_id n'est pas déjà dans les ID à prendre en compte)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "    # Maintenant, lancez k-NN sur le plus petit ensemble de vecs-à-considérer.\n",
    "    print(\"Une réflexion rapide %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    # convertir les vecs à considérer en une liste, puis en un tableau numpy\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    # appeler les voisins les plus proches sur la liste réduite des vecteurs candidats\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "\n",
    "    # Utiliser la liste d'index du plus proche voisin comme indices dans les ids à considérer\n",
    "    # créer une liste des voisins les plus proches par les identifiants des documents\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "UtTjmLKlsoRe"
   },
   "outputs": [],
   "source": [
    "#document_vecs, ind2Tweet\n",
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "nPtzTeYXsoRf",
    "outputId": "36b6b48b-e4e1-48c2-ab5d-ff67bc5b9b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suppréssion du doc_id 0 du vecteur d'entrée de new_ids_to_search\n",
      "suppréssion du doc_id 0 du vecteur d'entrée de new_ids_to_search\n",
      "suppréssion du doc_id 0 du vecteur d'entrée de new_ids_to_search\n",
      "suppréssion du doc_id 0 du vecteur d'entrée de new_ids_to_search\n",
      "suppréssion du doc_id 0 du vecteur d'entrée de new_ids_to_search\n",
      "Une réflexion rapide 77 vecs\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "nearest_neighbor_ids = approximate_knn(\n",
    "    doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "TIny-t8SsoRf",
    "outputId": "1d6f352c-12f0-4591-c153-0d98d2f092f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus proche voisin pour le document 0\n",
      "Contenue du document: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Plus proche voisin au document id :  2140\n",
      "Contenue du document: @PopsRamjet come one, every now and then is not so bad :)\n",
      "Plus proche voisin au document id :  701\n",
      "Contenue du document: With the top cutie of Bohol :) https://t.co/Jh7F6U46UB\n",
      "Plus proche voisin au document id :  51\n",
      "Contenue du document: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Plus proche voisin pour le document {doc_id}\")\n",
    "print(f\"Contenue du document: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Plus proche voisin au document id :  {neighbor_id}\")\n",
    "    print(f\"Contenue du document: {all_tweets[neighbor_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkNawuP7IhQt"
   },
   "source": [
    "# <a name=\"5\">V. Référence </a>\n",
    "\n",
    "- [Coursera Natural Language Processing with Classification and Vector Spaces](https://www.coursera.org/learn/classification-vector-spaces-in-nlp#syllabus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDpMuTH8soRf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "czFnmnLAsoQj"
   ],
   "name": "Modèle spatiaux vectoriel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
